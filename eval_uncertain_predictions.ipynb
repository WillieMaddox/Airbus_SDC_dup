{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage.util import montage\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from sdcdup.utils import overlap_tag_pairs\n",
    "from sdcdup.utils import generate_overlap_tag_slices\n",
    "from sdcdup.utils import generate_tag_pair_lookup\n",
    "from sdcdup.utils import channel_shift\n",
    "from sdcdup.utils import load_duplicate_truth\n",
    "\n",
    "from sdcdup.models.dupnet import load_checkpoint\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "EPS = np.finfo(np.float32).eps\n",
    "\n",
    "RED = (244, 67, 54)  #F44336 \n",
    "GREEN = (76, 175, 80)  #4CAF50 \n",
    "LIGHT_BLUE = (3, 169, 244)  #03A9F4\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 16\n",
    "BIGGEST_SIZE = 20\n",
    "plt.rc('font', size=BIGGEST_SIZE)         # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGEST_SIZE)    # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGEST_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGEST_SIZE)  # fontsize of the figure title\n",
    "\n",
    "montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n",
    "montage_pad = lambda x, *args, **kwargs: montage(x, padding_width=10, *args, **kwargs)\n",
    "zeros_mask = np.zeros((256*3, 256*3, 1), dtype=np.float32)\n",
    "\n",
    "# SENDTOENV\n",
    "train_image_dir = 'data/raw/train_768/'\n",
    "image_md5hash_grids_file = 'data/interim/image_md5hash_grids.pkl'\n",
    "image_bm0hash_grids_file = 'data/interim/image_bm0hash_grids.pkl'\n",
    "image_cm0hash_grids_file = 'data/interim/image_cm0hash_grids.pkl'\n",
    "image_greycop_grids_file = 'data/interim/image_greycop_grids.pkl'\n",
    "image_entropy_grids_file = 'data/interim/image_entropy_grids.pkl'\n",
    "image_issolid_grids_file = 'data/interim/image_issolid_grids.pkl'\n",
    "image_shipcnt_grids_file = 'data/interim/image_shipcnt_grids.pkl'\n",
    "\n",
    "overlap_tag_slices = generate_overlap_tag_slices()\n",
    "img_overlap_index_maps = generate_tag_pair_lookup()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENDTOMODULE\n",
    "class ImgMod:\n",
    "    \"\"\"\n",
    "    Reads a single image to be modified by hls.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.img_id = filename.split('/')[-1]\n",
    "\n",
    "        self._hls_chan = None\n",
    "        self._hls_gain = None\n",
    "\n",
    "        self._parent_bgr = None\n",
    "        self._parent_hls = None\n",
    "        self._parent_rgb = None\n",
    "        self._cv2_hls = None\n",
    "        self._cv2_bgr = None\n",
    "        self._cv2_rgb = None\n",
    "\n",
    "    def brightness_shift(self, chan, gain):\n",
    "        self._hls_chan = chan\n",
    "        self._hls_gain = gain\n",
    "        self._cv2_hls = None\n",
    "        return self.cv2_rgb\n",
    "    \n",
    "    def scale(self, minval, maxval):\n",
    "        m = 255.0 * (maxval - minval)\n",
    "        res = m * (self.parent_bgr - minval)\n",
    "        return np.around(res).astype(np.uint8)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.parent_bgr.shape\n",
    "    \n",
    "    @property\n",
    "    def parent_bgr(self):\n",
    "        if self._parent_bgr is None:\n",
    "            self._parent_bgr = cv2.imread(self.filename)\n",
    "        return self._parent_bgr\n",
    "\n",
    "    @property\n",
    "    def parent_hls(self):\n",
    "        if self._parent_hls is None:\n",
    "            self._parent_hls = self.to_hls(self.parent_bgr)\n",
    "        return self._parent_hls\n",
    "\n",
    "    @property\n",
    "    def parent_rgb(self):\n",
    "        if self._parent_rgb is None:\n",
    "            self._parent_rgb = self.to_rgb(self.parent_bgr)\n",
    "        return self._parent_rgb\n",
    "\n",
    "    @property\n",
    "    def cv2_hls(self):\n",
    "        if self._cv2_hls is None:\n",
    "            if self._hls_gain is None:\n",
    "                self._cv2_hls = self.parent_hls\n",
    "            else:\n",
    "                self._cv2_hls = channel_shift(self.parent_hls, self._hls_chan, self._hls_gain)\n",
    "        return self._cv2_hls\n",
    "\n",
    "    @property\n",
    "    def cv2_bgr(self):\n",
    "        if self._cv2_bgr is None:\n",
    "            self._cv2_bgr = self.to_bgr(self.cv2_hls)\n",
    "        return self._cv2_bgr\n",
    "\n",
    "    @property\n",
    "    def cv2_rgb(self):\n",
    "        if self._cv2_rgb is None:\n",
    "            self._cv2_rgb = self.to_rgb(self.cv2_bgr)\n",
    "        return self._cv2_rgb\n",
    "\n",
    "    def to_hls(self, bgr):\n",
    "        return cv2.cvtColor(bgr, cv2.COLOR_BGR2HLS_FULL)\n",
    "\n",
    "    def to_bgr(self, hls):\n",
    "        return cv2.cvtColor(hls, cv2.COLOR_HLS2BGR_FULL)\n",
    "\n",
    "    def to_rgb(self, bgr):\n",
    "        return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_truth = load_duplicate_truth()\n",
    "print(len(dup_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "def get_img(img_id):\n",
    "    return cv2.imread(os.path.join(train_image_dir, img_id))\n",
    "    \n",
    "# SENDTOMODULE\n",
    "class Dataset(data.Dataset):\n",
    "    \n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, tile_pairs, \n",
    "                 image_transform=None,\n",
    "                 in_shape=(6, 256, 256), \n",
    "                 out_shape=(1,)):\n",
    "\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.sz = 256\n",
    "        self.tile_pairs = tile_pairs\n",
    "        self.image_transform = image_transform\n",
    "        self.ij = ((0, 0), (0, 1), (0, 2),\n",
    "                   (1, 0), (1, 1), (1, 2),\n",
    "                   (2, 0), (2, 1), (2, 2))\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.tile_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        tp = self.tile_pairs[index]\n",
    "        \n",
    "        img1 = get_img(tp.img1_id)\n",
    "        img2 = get_img(tp.img2_id)\n",
    "        \n",
    "        tile1 = cv2.cvtColor(self.get_tile(img1, *self.ij[tp.idx1]), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.\n",
    "        tile2 = cv2.cvtColor(self.get_tile(img2, *self.ij[tp.idx2]), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.\n",
    "        \n",
    "        X = np.dstack([tile1, tile2])\n",
    "        X = X.transpose((2, 0, 1))\n",
    "        X = torch.from_numpy(X)\n",
    "        return X\n",
    "    \n",
    "    def get_tile(self, img, i, j):\n",
    "        return img[i * self.sz:(i + 1) * self.sz, j * self.sz:(j + 1) * self.sz, :]\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 6, 256, 256).to(device)\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TilePairs = namedtuple('TilePairs', 'img1_id img2_id img1_overlap_tag overlap_idx idx1 idx2')\n",
    "\n",
    "ytrue = []\n",
    "tile_pairs = []\n",
    "for (img1_id, img2_id, img1_overlap_tag), is_dup in tqdm_notebook(dup_truth.items()):\n",
    "    for overlap_idx, (idx1, idx2) in enumerate(img_overlap_index_maps[img1_overlap_tag]):\n",
    "        tile_pairs.append(TilePairs(img1_id, img2_id, img1_overlap_tag, overlap_idx, idx1, idx2))\n",
    "        ytrue.append(is_dup)\n",
    "print(len(tile_pairs), sum(ytrue), len(ytrue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Dataset(tile_pairs)\n",
    "test_dl = data.DataLoader(test_ds, batch_size=256, num_workers=18)\n",
    "test_dl = WrappedDataLoader(test_dl, preprocess)\n",
    "print(len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint('models/dup_model.2019_0802_2209.best.pth')\n",
    "model.cuda()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yprobs0 = [model(xb) for xb in tqdm_notebook(test_dl)]\n",
    "    yprobs = np.vstack([l.cpu() for l in yprobs0]).reshape(-1)\n",
    "    print(len(yprobs0), yprobs.shape, min(yprobs), max(yprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_cnn_tile_scores = {}\n",
    "for tp, yprob in zip(tile_pairs, yprobs):\n",
    "    \n",
    "    if (tp.img1_id, tp.img2_id) not in overlap_cnn_tile_scores:\n",
    "        overlap_cnn_tile_scores[(tp.img1_id, tp.img2_id)] = {}\n",
    "    \n",
    "    if tp.img1_overlap_tag not in overlap_cnn_tile_scores[(tp.img1_id, tp.img2_id)]:\n",
    "        n_overlapping_tiles = len(img_overlap_index_maps[tp.img1_overlap_tag])\n",
    "        cnn_scores = np.zeros(n_overlapping_tiles)\n",
    "        overlap_cnn_tile_scores[(tp.img1_id, tp.img2_id)][tp.img1_overlap_tag] = cnn_scores\n",
    "    \n",
    "    overlap_cnn_tile_scores[(tp.img1_id, tp.img2_id)][tp.img1_overlap_tag][tp.overlap_idx] = yprob\n",
    "\n",
    "DNN_Stats = namedtuple('dnn_stats', ['yprob', 'ypred', 'ytrue', 'loss', 'yconf'])\n",
    "\n",
    "dup_dict = {}\n",
    "for (img1_id, img2_id, img1_overlap_tag), ytrue in tqdm_notebook(dup_truth.items()):\n",
    "    assert img1_id < img2_id\n",
    "\n",
    "    dcnn_scores_raw = overlap_cnn_tile_scores[(img1_id, img2_id)][img1_overlap_tag]\n",
    "    dcnn_conf_raw = np.abs((dcnn_scores_raw - 0.5) * 2) # confidence? (1: very, 0: not at all)\n",
    "    yconf = np.min(dcnn_conf_raw)\n",
    "    yprob = np.min(dcnn_scores_raw)\n",
    "    ypred = (yprob > 0.5) * 1\n",
    "    assert ypred <= 1\n",
    "    \n",
    "    if ytrue:\n",
    "        bce = - ytrue * np.log(yprob)\n",
    "    else:\n",
    "        bce = - (1 - ytrue) * np.log(1 - yprob)\n",
    "    \n",
    "    dup_dict[(img1_id, img2_id, img1_overlap_tag)] = DNN_Stats(yprob, ypred, ytrue, bce, yconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_Stats2 = namedtuple('dnn_stats', ['key', 'yprob', 'ypred', 'ytrue', 'loss', 'yconf'])\n",
    "dup_dict_flat = []\n",
    "for keys, dnns in tqdm_notebook(dup_dict.items()):\n",
    "    dup_dict_flat.append(DNN_Stats2(keys, dnns.yprob, dnns.ypred, dnns.ytrue, dnns.loss, dnns.yconf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_confident = 0\n",
    "n_correct = 0\n",
    "id_tags = []\n",
    "for dnns in tqdm_notebook(sorted(dup_dict_flat, key=operator.attrgetter('yconf'), reverse=False)):\n",
    "\n",
    "    # Skip invalids, but print them out so we know which ones are.\n",
    "    if dnns.loss == np.nan:\n",
    "        print('nan ', dnns)\n",
    "        continue\n",
    "    if dnns.loss == np.inf:\n",
    "        print('+inf', dnns)\n",
    "        continue\n",
    "    if dnns.loss == -np.inf:\n",
    "        print('-inf', dnns)\n",
    "        continue\n",
    "        \n",
    "#     Skip the ones the dnn was certain about.\n",
    "#     if dnns.yconf > 0.02:\n",
    "#         n_confident += 1\n",
    "#         continue\n",
    "\n",
    "    # Skip the ones the dnn got correct.\n",
    "    if dnns.ypred == dnns.ytrue:\n",
    "        n_correct += 1\n",
    "        continue\n",
    "        \n",
    "    id_tags.append(dnns.key)\n",
    "len(id_tags), n_confident, n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_counter = Counter()\n",
    "for img1_id, img2_id, img1_overlap_tag in id_tags:\n",
    "    for overlap_idx, (idx1, idx2) in enumerate(img_overlap_index_maps[img1_overlap_tag]):\n",
    "        tags_counter[(img1_id, idx1)] += 1\n",
    "        tags_counter[(img2_id, idx2)] += 1\n",
    "print(len(tags_counter))\n",
    "\n",
    "for k, v in sorted(tags_counter.items(), key=operator.itemgetter(1), reverse=True):\n",
    "    if v > 3:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aa = 0\n",
    "n_samples = 10\n",
    "use_median_shift = True\n",
    "shift_brightness = False\n",
    "\n",
    "test_files = id_tags[aa * n_samples: (aa + 1) * n_samples]#[::-1]\n",
    "for f in test_files:\n",
    "    print(f, '{:10.5} {} {} {:10.5} {:.5} {}'.format(*dup_dict[f]))\n",
    "\n",
    "dtick = 256\n",
    "n_ticks = 768 // dtick + 1\n",
    "ticks = [i * dtick for i in range(n_ticks)]\n",
    "\n",
    "fig, m_axs = plt.subplots(n_samples, 2, figsize = (16, 8 * n_samples))\n",
    "for ii, (img1_id, img2_id, img1_overlap_tag) in enumerate(test_files):\n",
    "    \n",
    "    (ax1, ax2) = m_axs[ii]\n",
    "    yprob, ypred, is_dup, loss, yconf = dup_dict[(img1_id, img2_id, img1_overlap_tag)]\n",
    "    \n",
    "    imgmod1 = ImgMod(os.path.join(train_image_dir, img1_id))\n",
    "    imgmod2 = ImgMod(os.path.join(train_image_dir, img2_id))\n",
    "\n",
    "    slice1 = overlap_tag_slices[img1_overlap_tag]\n",
    "    slice2 = overlap_tag_slices[overlap_tag_pairs[img1_overlap_tag]]\n",
    "\n",
    "    m12 = np.median(np.vstack([imgmod1.parent_rgb[slice1], imgmod2.parent_rgb[slice2]]), axis=(0, 1), keepdims=True).astype(np.uint8)\n",
    "    \n",
    "    if shift_brightness:\n",
    "        brightness_level = -100 if np.sum(m12) >= 384 else 100\n",
    "        img1 = imgmod1.brightness_shift('L', brightness_level)\n",
    "        img2 = imgmod2.brightness_shift('L', brightness_level)\n",
    "    else:\n",
    "        img1 = imgmod1.parent_rgb\n",
    "        img2 = imgmod2.parent_rgb\n",
    "\n",
    "    if use_median_shift:\n",
    "        img1_drop = imgmod1.parent_rgb - m12\n",
    "        img2_drop = imgmod2.parent_rgb - m12\n",
    "    else:        \n",
    "        img1_drop = imgmod1.parent_rgb\n",
    "        img2_drop = imgmod2.parent_rgb\n",
    "    \n",
    "    img1[slice1] = img1_drop[slice1]\n",
    "    img2[slice2] = img2_drop[slice2]\n",
    "\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title(f'{img1_id}')\n",
    "    ax1.set_xticks(ticks)\n",
    "    ax1.set_yticks(ticks)\n",
    "\n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title(f'{img2_id}')\n",
    "    ax2.set_xticks(ticks)\n",
    "    ax2.set_yticks(ticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig.savefig(os.path.join('temp', BASE_MODEL, f\"{train_meta_filebase}_{score_str}_batch_{BATCH_NUM}_row_{aa+1}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
