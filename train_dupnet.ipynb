{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192555"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "from utils import channel_shift\n",
    "from utils import to_hls\n",
    "from utils import to_bgr\n",
    "\n",
    "from dupnet import even_split\n",
    "from dupnet import create_loss_and_optimizer\n",
    "from dupnet import save_checkpoint\n",
    "from dupnet import DupCNN\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ship_dir = \".data/input/\"\n",
    "train_768_dir = os.path.join('.data', 'train_768')\n",
    "train_256_dir = os.path.join(ship_dir, 'train_256')\n",
    "\n",
    "os.makedirs(train_256_dir, exist_ok=True)\n",
    "\n",
    "img_ids = os.listdir(train_768_dir)\n",
    "len(img_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_large(img, i, j):\n",
    "    tile = img[i * 256:(i + 1) * 256, j * 256:(j + 1) * 256, :]\n",
    "    tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    return tile\n",
    "\n",
    "def read_from_small(img_id, i, j):\n",
    "    filebase, fileext = img_id.split('.')\n",
    "    tile_id = f'{filebase}_{i}{j}.{fileext}'\n",
    "    tile = cv2.imread(os.path.join(train_256_dir, tile_id))\n",
    "    tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    return tile\n",
    "\n",
    "def from_large(img_id, i, j, k, l):\n",
    "    img = cv2.imread(os.path.join(train_768_dir, img_id))\n",
    "    tile1 = read_from_large(img, i, j)\n",
    "    tile2 = read_from_large(img, k, l)\n",
    "    return np.sum(tile1 - tile2)\n",
    "\n",
    "def from_small(img_id, i, j, k, l):\n",
    "    tile1 = read_from_small(img_id, i, j)\n",
    "    tile2 = read_from_small(img_id, k, l)\n",
    "    return np.sum(tile1 - tile2)\n",
    "\n",
    "def from_both(img_id, i, j, k, l):\n",
    "    img = cv2.imread(os.path.join(train_768_dir, img_id))\n",
    "    tile1 = read_from_large(img, i, j)\n",
    "    tile2 = read_from_small(img_id, k, l)\n",
    "    return np.sum(tile1 - tile2)\n",
    "\n",
    "n_steps = 100\n",
    "\n",
    "ij_pairs = [(0, 0), (0, 1), (0, 2),\n",
    "            (1, 0), (1, 1), (1, 2),\n",
    "            (2, 0), (2, 1), (2, 2)]\n",
    "ijkl = []\n",
    "for i in range(n_steps):\n",
    "    ij, kl = np.random.choice(len(ij_pairs), 2, replace=False)\n",
    "    i, j = ij_pairs[ij]\n",
    "    k, l = ij_pairs[kl]\n",
    "    ijkl.append((i, j, k, l))\n",
    "\n",
    "img_id_list = np.random.choice(img_ids, n_steps)\n",
    "t0 = time.time()\n",
    "for (i, j, k, l), img_id in zip(ijkl, img_id_list):\n",
    "    _ = from_large(img_id, i, j, k, l)\n",
    "print(time.time() - t0)\n",
    "\n",
    "img_id_list = np.random.choice(img_ids, n_steps)\n",
    "t0 = time.time()\n",
    "for (i, j, k, l), img_id in zip(ijkl, img_id_list):\n",
    "    _ = from_small(img_id, i, j, k, l)\n",
    "print(time.time() - t0)\n",
    "\n",
    "img_id_list = np.random.choice(img_ids, n_steps)\n",
    "t0 = time.time()\n",
    "for (i, j, k, l), img_id in zip(ijkl, img_id_list):\n",
    "    _ = from_both(img_id, i, j, k, l)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_chan_map = {0: 'H', 1: 'L', 2: 'S'}\n",
    "\n",
    "ImgAugs = namedtuple('ImgAugs', 'idx0 idx1 ij kl chan gain')\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, img_ids, train_768_dir, train_256_dir, \n",
    "                 in_shape=(6, 256, 256), \n",
    "                 out_shape=(1,), \n",
    "                 valid=False):\n",
    "        'Initialization'\n",
    "        self.img_ids = img_ids\n",
    "        self.train_768_dir = train_768_dir\n",
    "        self.train_256_dir = train_256_dir\n",
    "        self.ij = [(0, 0), (0, 1), (0, 2),\n",
    "                   (1, 0), (1, 1), (1, 2),\n",
    "                   (2, 0), (2, 1), (2, 2)]\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        self.hls_limits = {'H': 5, 'L': 15, 'S': 15}\n",
    "        \n",
    "        self.valid = valid\n",
    "        if self.valid:\n",
    "            self.img_augs = {}\n",
    "            for i, img_id in enumerate(self.img_ids):\n",
    "                self.img_augs[img_id] = self.get_random_mapping()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        img_id = self.img_ids[index]\n",
    "        if self.valid:\n",
    "            img_aug = self.img_augs[img_id]\n",
    "        else:\n",
    "            img_aug = self.get_random_mapping()\n",
    "        X, y = self.get_data_pair(img_id, img_aug)\n",
    "        return X, y\n",
    "    \n",
    "    def get_random_mapping(self):\n",
    "        idx0 = np.random.choice(3, p=[0.5, 0.5, 0.0])\n",
    "        idx1 = np.random.choice(4, p=[0.25, 0.25, 0.25, 0.25])\n",
    "        ij, kl = np.random.choice(len(self.ij), 2, replace=False)\n",
    "        hls_idx = np.random.choice(3)\n",
    "        hls_chan = idx_chan_map[hls_idx]\n",
    "        hls_gain = np.random.choice(self.hls_limits[hls_chan]) + 1\n",
    "        hls_gain = hls_gain if np.random.random() > 0.5 else -1 * hls_gain\n",
    "        \n",
    "        return ImgAugs(idx0, idx1, ij, kl, hls_chan, hls_gain)\n",
    "    \n",
    "    def color_shift(self, img, chan, gain):\n",
    "        hls = to_hls(img)\n",
    "        hls_shifted = channel_shift(hls, chan, gain)\n",
    "        return to_bgr(hls_shifted)\n",
    "    \n",
    "    def get_tile(self, img, i, j, sz=256):\n",
    "        return img[i * sz:(i + 1) * sz, j * sz:(j + 1) * sz, :]\n",
    "    \n",
    "    def read_from_large(self, img_id, i, j):\n",
    "        img = cv2.imread(os.path.join(self.train_768_dir, img_id))\n",
    "        return self.get_tile(img, i, j)\n",
    "    \n",
    "    def read_from_small(self, img_id, i, j):\n",
    "        filebase, fileext = img_id.split('.')\n",
    "        tile_id = f'{filebase}_{i}{j}.{fileext}'\n",
    "        return cv2.imread(os.path.join(self.train_256_dir, tile_id))\n",
    "    \n",
    "    def get_data_pair(self, img_id, img_aug):\n",
    "\n",
    "        # same img_id (img_id1 == img_id2), same tile (ij == kl)\n",
    "            # img_m[i,j], img_m[i,j], 1, exact\n",
    "            # img_m[i,j], tile_m_ij, 1, fuzzy\n",
    "            # tile_m_ij, img_m[i,j], 1, fuzzy\n",
    "            # tile_m_ij, tile_m_ij, 1, exact\n",
    "        # same img_id (img_id1 == img_id2), diff tile (ij != kl)\n",
    "            # img_m[i,j], img_m[k,l], 0, similar but different\n",
    "            # img_m[i,j], tile_m_kl, 0, similar but different\n",
    "            # tile_m_ij, img_m[k,l], 0, similar but different\n",
    "            # tile_m_ij, tile_m_kl, 0, similar but different\n",
    "        # diff img_id (img_id1 != img_id2), same tile (ij == kl)\n",
    "            # img_m[i,j], img_n[i,j], 0, very different\n",
    "            # img_m[i,j], tile_n_ij, 0, very different\n",
    "            # tile_m_ij, img_n[i,j], 0, very different\n",
    "            # tile_m_ij, tile_n_ij, 0, very different\n",
    "        # diff img_id (img_id1 != img_id2), diff tile (ij != kl)\n",
    "            # img_m[i,j], img_n[k,l], 0, very different\n",
    "            # img_m[i,j], tile_n_kl, 0, very different\n",
    "            # tile_m_ij, img_n[k,l], 0, very different\n",
    "            # tile_m_ij, tile_n_kl, 0, very different\n",
    "        \n",
    "        idx0, idx1, ij, kl, chan, gain = img_aug\n",
    "        i, j = self.ij[ij]\n",
    "        k, l = self.ij[kl]\n",
    "        \n",
    "        if idx0 == 0:\n",
    "            \n",
    "            if idx1 == 0:\n",
    "                tile1 = self.read_from_large(img_id, i, j)\n",
    "                tile2 = self.color_shift(tile1, chan, gain)\n",
    "            elif idx1 == 1:\n",
    "                tile1 = self.read_from_large(img_id, i, j)\n",
    "                tile2 = self.read_from_small(img_id, i, j)\n",
    "            elif idx1 == 2:\n",
    "                tile1 = self.read_from_small(img_id, i, j)\n",
    "                tile2 = self.read_from_large(img_id, i, j)\n",
    "            elif idx1 == 3:\n",
    "                tile1 = self.read_from_small(img_id, i, j)\n",
    "                tile2 = self.color_shift(tile1, chan, gain)\n",
    "            else:\n",
    "                raise ValueError\n",
    "            \n",
    "            y = 1\n",
    "            \n",
    "        elif idx0 == 1:\n",
    "            \n",
    "            # These 4 have pretty much the same effect.\n",
    "            # The first one is the fastest.\n",
    "#             idx1 = np.random.choice(4, p=[0.1, 0.1, 0.1, 1.7])\n",
    "            idx1 = 0\n",
    "            if idx1 == 0:\n",
    "                img = cv2.imread(os.path.join(self.train_768_dir, img_id))\n",
    "                tile1 = self.get_tile(img, i, j)\n",
    "                tile2 = self.get_tile(img, k, l)\n",
    "            elif idx1 == 1:\n",
    "                tile1 = self.read_from_large(img_id, i, j)\n",
    "                tile2 = self.read_from_small(img_id, k, l)\n",
    "            elif idx1 == 2:\n",
    "                tile1 = self.read_from_small(img_id, i, j)\n",
    "                tile2 = self.read_from_large(img_id, k, l)\n",
    "            elif idx1 == 3:\n",
    "                tile1 = self.read_from_small(img_id, i, j)\n",
    "                tile2 = self.read_from_small(img_id, k, l)\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            y = 0\n",
    "\n",
    "        elif idx0 == 2:\n",
    "\n",
    "            img_id2 = np.random.choice(self.img_ids)\n",
    "            \n",
    "            idx1 = np.random.choice(2)\n",
    "            if idx1 == 0:\n",
    "                tile1 = self.read_from_small(img_id, i, j)\n",
    "                tile2 = self.read_from_small(img_id2, i, j)\n",
    "            elif idx1 == 1:\n",
    "                tile1 = self.read_from_small(img_id, i, j)\n",
    "                tile2 = self.read_from_small(img_id2, k, l)\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            y = 0\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        if np.all(tile1 == tile2) and y == 0:\n",
    "            # print(f'{img_id}, ({i}, {j}), ({k}, {l}) correcting...')\n",
    "            y = 1\n",
    "        \n",
    "        tile1 = cv2.cvtColor(tile1, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.\n",
    "        tile2 = cv2.cvtColor(tile2, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.\n",
    "        \n",
    "        X = np.dstack([tile1, tile2])\n",
    "#         X = tile1 - tile2\n",
    "        X = X.transpose((2, 0, 1))\n",
    "        y = np.array([y], dtype=np.float32)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459 45\n"
     ]
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "input_shape = (6, 256, 256)\n",
    "conv_layers = (16, 32, 64, 128, 256)\n",
    "fc_layers = (128,)\n",
    "output_size = 1\n",
    "\n",
    "# Parameters\n",
    "split = 80\n",
    "batch_size = 256\n",
    "max_epochs = 30\n",
    "num_workers = 18\n",
    "learning_rate = 0.00001\n",
    "\n",
    "train_params = {'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'num_workers': num_workers}\n",
    "\n",
    "valid_params = {'batch_size': batch_size,\n",
    "                'shuffle': False,\n",
    "                'num_workers': num_workers}\n",
    "# Datasets\n",
    "np.random.shuffle(img_ids)\n",
    "n_train, n_valid = even_split(len(img_ids), batch_size, split)\n",
    "partition = {'train': img_ids[:n_train], 'valid': img_ids[-n_valid:]}\n",
    "\n",
    "# Generators\n",
    "train_set = Dataset(partition['train'], train_768_dir, train_256_dir)\n",
    "train_generator = data.DataLoader(train_set, **train_params)\n",
    "\n",
    "valid_set = Dataset(partition['valid'], train_768_dir, train_256_dir, valid=True)\n",
    "valid_generator = data.DataLoader(valid_set, **valid_params)\n",
    "print(len(train_generator), len(valid_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(train_generator))\n",
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 6, 256, 256]), torch.Size([128, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = next(iter(valid_generator))\n",
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = DupCNN(input_shape, output_size, conv_layers, fc_layers)\n",
    "model.cuda()\n",
    "\n",
    "loss, optimizer = create_loss_and_optimizer(model, learning_rate)\n",
    "n_batches = len(train_generator)\n",
    "best_loss = 9999.0\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    t = tnrange(len(train_generator))\n",
    "    train_iterator = iter(train_generator)\n",
    "    for i in t:\n",
    "        t.set_description(f'Epoch {epoch + 1:>3}')\n",
    "        inputs, labels = train_iterator.next()\n",
    "        # Transfer to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        #Set the parameter gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Forward pass, backward pass, optimize\n",
    "        outputs = model(inputs)\n",
    "        loss_size = loss(outputs, labels)\n",
    "        loss_size.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Print statistics\n",
    "        total_train_loss += loss_size.data.item()\n",
    "        \n",
    "        y_pred = outputs > 0.5\n",
    "        y_pred = y_pred.type_as(torch.cuda.FloatTensor())\n",
    "        equality = labels == y_pred\n",
    "        total_train_acc += equality.type_as(torch.FloatTensor()).mean()\n",
    "        \n",
    "        loss_str = f'{total_train_loss/(i + 1):.6f}'\n",
    "        acc_str = f'{total_train_acc/(i + 1):.5f}'\n",
    "        t.set_postfix(loss=loss_str, acc=acc_str)\n",
    "            \n",
    "    # Validation\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    t = tnrange(len(valid_generator))\n",
    "    valid_iterator = iter(valid_generator)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i in t:\n",
    "            t.set_description(f'Epoch {epoch + 1:>3}')\n",
    "            inputs, labels = valid_iterator.next()\n",
    "            # Transfer to GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            #Forward pass\n",
    "            val_outputs = model(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data.item()\n",
    "\n",
    "            y_pred = val_outputs > 0.5\n",
    "            y_pred = y_pred.type_as(torch.cuda.FloatTensor())\n",
    "            equality = labels == y_pred\n",
    "            total_val_acc += equality.type_as(torch.FloatTensor()).mean()\n",
    "        \n",
    "            loss_str = f'{total_val_loss/(i + 1):.6f}'\n",
    "            acc_str = f'{total_val_acc/(i + 1):.5f}'\n",
    "            t.set_postfix(loss=loss_str, acc=acc_str)\n",
    "\n",
    "        val_loss = total_val_loss/(i + 1)\n",
    "        if val_loss < best_loss:\n",
    "            save_checkpoint(os.path.join(\"out\", f\"dup_model.{epoch + 1:03d}-{val_loss:.5f}.pth\"), model)\n",
    "            save_checkpoint(os.path.join(\"out\", \"dup_model.last.pth\"), model)\n",
    "#             save_checkpoint(''.join(['out/checkpoint_', acc_str, '.pth']), model)\n",
    "            best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
