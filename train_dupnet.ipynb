{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "from tqdm import tnrange\n",
    "import numpy as np\n",
    "import cv2\n",
    "from parse import parse\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms\n",
    "\n",
    "from sdcdup.utils import get_datetime_now\n",
    "from sdcdup.utils import channel_shift\n",
    "from sdcdup.utils import to_hls\n",
    "from sdcdup.utils import to_bgr\n",
    "from sdcdup.utils import create_dataset_from_tiles_and_truth\n",
    "from sdcdup.utils import even_split\n",
    "\n",
    "from test_friend_circles import SDCImageContainer\n",
    "from datasets import create_dataset_from_truth\n",
    "from dupnet import create_loss_and_optimizer\n",
    "from dupnet import save_checkpoint\n",
    "from dupnet import DupCNN\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ship_dir = \"data/input/\"\n",
    "train_768_dir = os.path.join(\"data\", \"train_768\")\n",
    "train_256_dir = os.path.join(ship_dir, \"train_256\")\n",
    "train_image_dir = os.path.join(ship_dir, \"train_768\")\n",
    "image_md5hash_grids_file = os.path.join(\"data\", \"image_md5hash_grids.pkl\")\n",
    "image_bm0hash_grids_file = os.path.join(\"data\", \"image_bm0hash_grids.pkl\")\n",
    "image_cm0hash_grids_file = os.path.join(\"data\", \"image_cm0hash_grids.pkl\")\n",
    "image_greycop_grids_file = os.path.join(\"data\", \"image_greycop_grids.pkl\")\n",
    "image_entropy_grids_file = os.path.join(\"data\", \"image_entropy_grids.pkl\")\n",
    "image_issolid_grids_file = os.path.join(\"data\", \"image_issolid_grids.pkl\")\n",
    "duplicate_truth_file = os.path.join(\"data\", \"duplicate_truth.txt\")\n",
    "os.makedirs(train_256_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalFlip:\n",
    "    \"\"\"Horizontally flip the given numpy array randomly with a given probability.\n",
    "\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Image): Image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.p:\n",
    "            return cv2.flip(img, 1)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "\n",
    "class RandomTransformC4:\n",
    "    \"\"\"Rotate a n-D tensor by 90 degrees in the H x W plane.\n",
    "\n",
    "    Args:\n",
    "        with_identity (bool): whether or not to include 0 degrees as a probable rotation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, with_identity=True):\n",
    "        self.with_identity = with_identity\n",
    "        self.n90s = (0, 1, 2, 3) if self.with_identity else (1, 2, 3)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Image): Image to be rotated.\n",
    "\n",
    "        Returns:\n",
    "            Image: Randomly rotated image but in 90 degree increments.\n",
    "        \"\"\"\n",
    "        k = random.choice(self.n90s)\n",
    "        return torch.rot90(img, k, (1, 2))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(with_identity={})'.format(self.with_identity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "idx_chan_map = {0: 'H', 1: 'L', 2: 'S'}\n",
    "\n",
    "ImgAugs = namedtuple('ImgAugs', 'idx3 chan gain')\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, img_overlaps, train_768_dir, train_256_dir, train_or_valid, image_transform,\n",
    "                 in_shape=(6, 256, 256), \n",
    "                 out_shape=(1,)):\n",
    "        \n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.img_overlaps = img_overlaps\n",
    "        self.train_768_dir = train_768_dir\n",
    "        self.train_256_dir = train_256_dir\n",
    "        # TODO: handle case if train_or_valid == 'test'\n",
    "        self.valid = train_or_valid == 'valid'\n",
    "        self.image_transform = image_transform\n",
    "        self.ij = ((0, 0), (0, 1), (0, 2),\n",
    "                   (1, 0), (1, 1), (1, 2),\n",
    "                   (2, 0), (2, 1), (2, 2))\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        self.hls_limits = {'H': 5, 'L': 10, 'S': 10}\n",
    "        if self.valid:\n",
    "            self.img_augs = [self.get_random_augmentation() for _ in self.img_overlaps]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.img_overlaps)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        if self.valid:\n",
    "            img_aug = self.img_augs[index]\n",
    "        else:\n",
    "            img_aug = self.get_random_augmentation()\n",
    "        X, y = self.get_data_pair(self.img_overlaps[index], img_aug)\n",
    "        return X, y\n",
    "    \n",
    "    def get_random_augmentation(self):\n",
    "\n",
    "        idx3 = np.random.choice(4, p=[0.4, 0.1, 0.1, 0.4])\n",
    "        \n",
    "        hls_idx = np.random.choice(3)\n",
    "        hls_chan = idx_chan_map[hls_idx]\n",
    "        hls_gain = np.random.choice(self.hls_limits[hls_chan]) + 1\n",
    "        hls_gain = hls_gain if np.random.random() > 0.5 else -1 * hls_gain\n",
    "        \n",
    "        return ImgAugs(idx3, hls_chan, hls_gain)\n",
    "    \n",
    "    def color_shift(self, img, chan, gain):\n",
    "        hls = to_hls(img)\n",
    "        hls_shifted = channel_shift(hls, chan, gain)\n",
    "        return to_bgr(hls_shifted)\n",
    "    \n",
    "    def get_tile(self, img, ij, sz=256):\n",
    "        i, j = self.ij[ij]\n",
    "        return img[i * sz:(i + 1) * sz, j * sz:(j + 1) * sz, :]\n",
    "    \n",
    "    def read_from_large(self, img_id, ij):\n",
    "        img = cv2.imread(os.path.join(self.train_768_dir, img_id))\n",
    "        return self.get_tile(img, ij)\n",
    "    \n",
    "    def read_from_small(self, img_id, ij):\n",
    "        filebase, fileext = img_id.split('.')\n",
    "        tile_id = f'{filebase}_{ij}.{fileext}'\n",
    "        return cv2.imread(os.path.join(self.train_256_dir, tile_id))\n",
    "    \n",
    "    def get_data_pair(self, img_overlap, img_aug):\n",
    "\n",
    "        # diff img_id (img1_id != img2_id), random tile from overlap, where is_dup == 1 (from duplicate_truth.txt)\n",
    "            # img1_[i,j], img2_[k,l], 1, exact or fuzzy\n",
    "            # img1_[i,j], tile2_kl, 1, exact or fuzzy\n",
    "            # tile1_ij, img2_[k,l], 1, exact or fuzzy\n",
    "            # tile1_ij, tile2_kl, 1, exact or fuzzy\n",
    "        \n",
    "        # same img_id (img1_id == img2_id), same tile (ij == kl)\n",
    "            # img1_[i,j], img1_[i,j], 1, exact\n",
    "            # img1_[i,j], tile1_ij, 1, fuzzy\n",
    "            # tile1_ij, img1_[i,j], 1, fuzzy\n",
    "            # tile1_ij, tile1_ij, 1, exact\n",
    "            \n",
    "        # same img_id (img1_id == img2_id), diff tile (ij != kl)\n",
    "            # img1_[i,j], img1_[k,l], 0, similar but different\n",
    "            # img1_[i,j], tile1_kl, 0, similar but different\n",
    "            # tile1_ij, img1_[k,l], 0, similar but different\n",
    "            # tile1_ij, tile1_kl, 0, similar but different\n",
    "            \n",
    "        # diff img_id (img1_id != img2_id), same tile (ij == kl)\n",
    "            # img1_[i,j], img2_[i,j], 0, very different\n",
    "            # img1_[i,j], tile2_ij, 0, very different\n",
    "            # tile1_ij, img2_[i,j], 0, very different\n",
    "            # tile1_ij, tile2_ij, 0, very different\n",
    "            \n",
    "        # diff img_id (img1_id != img2_id), diff tile (ij != kl)\n",
    "            # img1_[i,j], img2_[k,l], 0, very different\n",
    "            # img1_[i,j], tile2_kl, 0, very different\n",
    "            # tile1_ij, img2_[k,l], 0, very different\n",
    "            # tile1_ij, tile2_kl, 0, very different\n",
    "        \n",
    "        # use image_md5hash_grids.pkl for equal image id pairs (img1_id == img2_id)\n",
    "        #--------------------------------------------------------------------\n",
    "        # ij == kl? | tile1? | tile2? | shift? | is_dup?\n",
    "        #--------------------------------------------------------------------\n",
    "        #   yes     |  768   |  768   |   yes  |    yes agro color shift \n",
    "        #   yes     |  768   |  768   |    no  |    yes\n",
    "        #   yes     |  768   |  256   |    no  |    yes \n",
    "        #   yes     |  256   |  768   |    no  |    yes \n",
    "        #   yes     |  256   |  256   |   yes  |    yes agro color shift \n",
    "        #   yes     |  256   |  256   |    no  |    yes \n",
    "        #    no     |  768   |  768   |   yes  |     no \n",
    "        #    no     |  768   |  768   |    no  |     no \n",
    "        #    no     |  256   |  256   |   yes  |     no \n",
    "        #    no     |  256   |  256   |    no  |     no \n",
    "        \n",
    "        # use duplicate_truth.txt for unequal image id pairs (img1_id != img2_id)\n",
    "        # NOTE: Be sure to use the overlap_map when comparing ij and kl\n",
    "        #--------------------------------------------------------------------\n",
    "        # ij == kl? | tile1? | tile2? | shift? | is_dup?\n",
    "        #--------------------------------------------------------------------\n",
    "        #   yes     |  768   |  768   |   yes  |    yes small color shift \n",
    "        #   yes     |  768   |  768   |    no  |    yes\n",
    "        #   yes     |  768   |  256   |    no  |    yes\n",
    "        #   yes     |  256   |  768   |    no  |    yes\n",
    "        #   yes     |  256   |  256   |   yes  |    yes\n",
    "        #   yes     |  256   |  256   |    no  |    yes\n",
    "        #    no     |  768   |  768   |   yes  |     no\n",
    "        #    no     |  768   |  768   |    no  |     no\n",
    "        #    no     |  256   |  256   |   yes  |     no \n",
    "        #    no     |  256   |  256   |    no  |     no\n",
    "\n",
    "        img1_id, img2_id, idx1, idx2, is_dup = img_overlap\n",
    "        idx3, chan, gain = img_aug\n",
    "\n",
    "        if img1_id == img2_id:\n",
    "            if is_dup:  # ij == kl\n",
    "                if idx3 == 0:\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.color_shift(tile1, chan, gain)\n",
    "                elif idx3 == 1:\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.read_from_small(img2_id, idx2)\n",
    "                elif idx3 == 2:\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.read_from_large(img2_id, idx2)\n",
    "                elif idx3 == 3:\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.color_shift(tile1, chan, gain)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            else:  # ij != kl\n",
    "                # These 4 have pretty much the same effect.\n",
    "                # The last one (2 256x256 tiles vs 1 768x768 tile) is the fastest.\n",
    "                # idx3 = np.random.choice(4, p=[0.1, 0.1, 0.1, 1.7])\n",
    "                # idx3 = 3\n",
    "                if idx3 == 0: # fast\n",
    "                    img = cv2.imread(os.path.join(self.train_768_dir, img1_id))\n",
    "                    tile1 = self.get_tile(img, idx1)\n",
    "                    tile2 = self.get_tile(img, idx2)\n",
    "                elif idx3 == 1: # slowest\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.read_from_small(img2_id, idx2)\n",
    "                elif idx3 == 2: # slowest\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.read_from_large(img2_id, idx2)\n",
    "                elif idx3 == 3: # fastest\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.read_from_small(img2_id, idx2)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "        else: # img1_id != img2_id\n",
    "            if is_dup:  # ij == kl\n",
    "                if idx3 == 0: # slowest\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.read_from_large(img2_id, idx2)\n",
    "                elif idx3 == 1: # slow\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.read_from_small(img2_id, idx2)\n",
    "                elif idx3 == 2: # slow\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.read_from_large(img2_id, idx2)\n",
    "                elif idx3 == 3: # fast\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.read_from_small(img2_id, idx2)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            else:  # ij != kl\n",
    "                tile1 = self.read_from_large(img1_id, idx1)\n",
    "                tile2 = self.read_from_large(img2_id, idx2)\n",
    "#                 if idx3 == 0: # slowest\n",
    "#                     tile1 = self.read_from_large(img1_id, i, j)\n",
    "#                     tile2 = self.read_from_large(img2_id, k, l)\n",
    "#                 elif idx3 == 1: # slow\n",
    "#                     tile1 = self.read_from_large(img1_id, i, j)\n",
    "#                     tile2 = self.read_from_small(img2_id, k, l)\n",
    "#                 elif idx3 == 2: # slow\n",
    "#                     tile1 = self.read_from_small(img1_id, i, j)\n",
    "#                     tile2 = self.read_from_large(img2_id, k, l)\n",
    "#                 elif idx3 == 3: # fast\n",
    "#                     tile1 = self.read_from_small(img1_id, i, j)\n",
    "#                     tile2 = self.read_from_small(img2_id, k, l)\n",
    "#                 else:\n",
    "#                     raise ValueError\n",
    "\n",
    "        if is_dup == 0 and sdcic.tile_md5hash_grids[img1_id][idx1] == sdcic.tile_md5hash_grids[img2_id][idx2]:\n",
    "            i, j = self.ij[idx1]\n",
    "            k, l = self.ij[idx2]\n",
    "            print(f'algo={idx3}; {img1_id} {idx1} -> ({i},{j}); {img2_id}, {idx2} -> ({k},{l}); correcting... {is_dup} -> 1')\n",
    "            is_dup = 1\n",
    "            \n",
    "        tile1 = cv2.cvtColor(tile1, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.\n",
    "        tile2 = cv2.cvtColor(tile2, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.\n",
    "        \n",
    "        X = np.dstack([tile1, tile2]) if np.random.random() < 0.5 else np.dstack([tile2, tile1])\n",
    "        X = self.image_transform(X)\n",
    "        y = np.array([is_dup], dtype=np.float32)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdcic = SDCImageContainer(train_image_dir)\n",
    "sdcic.preprocess_image_properties(\n",
    "    image_md5hash_grids_file,\n",
    "    image_bm0hash_grids_file,\n",
    "    image_cm0hash_grids_file,\n",
    "    image_greycop_grids_file,\n",
    "    image_entropy_grids_file,\n",
    "    image_issolid_grids_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "idx_chan_map = {0: 'H', 1: 'L', 2: 'S'}\n",
    "\n",
    "ImgAugs = namedtuple('ImgAugs', 'idx3 chan gain')\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, img_overlaps, train_or_valid, image_transform,\n",
    "                 in_shape=(6, 256, 256), \n",
    "                 out_shape=(1,)):\n",
    "        \n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.img_overlaps = img_overlaps\n",
    "        # TODO: handle case if train_or_valid == 'test'\n",
    "        self.valid = train_or_valid == 'valid'\n",
    "        self.image_transform = image_transform\n",
    "        self.ij = ((0, 0), (0, 1), (0, 2),\n",
    "                   (1, 0), (1, 1), (1, 2),\n",
    "                   (2, 0), (2, 1), (2, 2))\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        self.hls_limits = {'H': 5, 'L': 10, 'S': 10}\n",
    "        if self.valid:\n",
    "            self.img_augs = [self.get_random_augmentation() for _ in self.img_overlaps]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.img_overlaps)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        if self.valid:\n",
    "            img_aug = self.img_augs[index]\n",
    "        else:\n",
    "            img_aug = self.get_random_augmentation()\n",
    "        X, y = self.get_data_pair(self.img_overlaps[index], img_aug)\n",
    "        return X, y\n",
    "    \n",
    "    def get_random_augmentation(self):\n",
    "\n",
    "#         p = [0.4, 0.1, 0.1, 0.4]\n",
    "#         p = [0.1, 0.05, 0.05, 0.8]\n",
    "        p = [0.3, 0.2, 0.2, 0.3]\n",
    "        idx3 = np.random.choice(4, p=p)\n",
    "        \n",
    "        hls_idx = np.random.choice(3)\n",
    "        hls_chan = idx_chan_map[hls_idx]\n",
    "        hls_gain = np.random.choice(self.hls_limits[hls_chan]) + 1\n",
    "        hls_gain = hls_gain if np.random.random() > 0.5 else -1 * hls_gain\n",
    "        \n",
    "        return ImgAugs(idx3, hls_chan, hls_gain)\n",
    "    \n",
    "    def color_shift(self, img, chan, gain):\n",
    "        hls = to_hls(img)\n",
    "        hls_shifted = channel_shift(hls, chan, gain)\n",
    "        return to_bgr(hls_shifted)\n",
    "    \n",
    "    def get_tile(self, img, idx, sz=256):\n",
    "        i, j = self.ij[idx]\n",
    "        return img[i * sz:(i + 1) * sz, j * sz:(j + 1) * sz, :]\n",
    "    \n",
    "    def read_from_large(self, img_id, idx):\n",
    "#         if not os.path.exists(img_id):\n",
    "#             print(img_id, idx)\n",
    "        img = cv2.imread(img_id)\n",
    "        return self.get_tile(img, idx)\n",
    "    \n",
    "    def read_from_small(self, img_id, idx):\n",
    "        dup_truth_path, img_filename = img_id.rsplit(\"/images_768/\")\n",
    "        row, col = parse('r{:3d}_c{:3d}.jpg', img_filename)\n",
    "        i, j = self.ij[idx]\n",
    "        tile_id = os.path.join(dup_truth_path, \"images_256\", f\"r{row + i:03d}_c{col + j:03d}.jpg\")\n",
    "#         if not os.path.exists(tile_id):\n",
    "#             print(img_id, idx)\n",
    "#             print(tile_id)\n",
    "        return cv2.imread(tile_id)\n",
    "    \n",
    "    def get_data_pair(self, img_overlap, img_aug):\n",
    "\n",
    "        img1_id, img2_id, idx1, idx2, is_dup = img_overlap\n",
    "        idx3, chan, gain = img_aug\n",
    "        same_image = img1_id == img2_id\n",
    "        \n",
    "        if same_image:\n",
    "            if is_dup:  # idx1 == idx2\n",
    "                if idx3 == 0:\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.color_shift(tile1, chan, gain)\n",
    "                elif idx3 == 1:\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.read_from_small(img2_id, idx2)\n",
    "                elif idx3 == 2:\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.read_from_large(img2_id, idx2)\n",
    "                elif idx3 == 3:\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.color_shift(tile1, chan, gain)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            else:  # idx1 != idx2\n",
    "                # idx3 = 3\n",
    "                if idx3 == 0: # fast\n",
    "                    img = cv2.imread(img1_id)\n",
    "                    tile1 = self.get_tile(img, idx1)\n",
    "                    tile2 = self.get_tile(img, idx2)\n",
    "                elif idx3 == 1: # slowest\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.read_from_small(img2_id, idx2)\n",
    "                elif idx3 == 2: # slowest\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.read_from_large(img2_id, idx2)\n",
    "                elif idx3 == 3: # fastest\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.read_from_small(img2_id, idx2)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "        else: # img1_id != img2_id\n",
    "            if is_dup:  # idx1 == idx2\n",
    "                if idx3 == 0: # slowest\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.read_from_large(img2_id, idx2)\n",
    "                elif idx3 == 1: # slow\n",
    "                    tile1 = self.read_from_large(img1_id, idx1)\n",
    "                    tile2 = self.read_from_small(img2_id, idx2)\n",
    "                elif idx3 == 2: # slow\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.read_from_large(img2_id, idx2)\n",
    "                elif idx3 == 3: # fast\n",
    "                    # These end up being the same tiles.\n",
    "                    tile1 = self.read_from_small(img1_id, idx1)\n",
    "                    tile2 = self.color_shift(tile1, chan, gain)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "            else:  # idx1 != idx2\n",
    "                tile1 = self.read_from_large(img1_id, idx1)\n",
    "                tile2 = self.read_from_large(img2_id, idx2)\n",
    "\n",
    "#         i, j = self.ij[idx1]\n",
    "#         k, l = self.ij[idx2]\n",
    "#         print(f'same_image, is_dup, idx3: {same_image*1}, {is_dup}, {idx3}\\n{img1_id} {idx1} -> ({i},{j})\\n{img2_id} {idx2} -> ({k},{l})\\n')\n",
    "        tile1 = cv2.cvtColor(tile1, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.\n",
    "        tile2 = cv2.cvtColor(tile2, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.\n",
    "        \n",
    "        X = np.dstack([tile1, tile2]) if np.random.random() < 0.5 else np.dstack([tile2, tile1])\n",
    "        X = self.image_transform(X)\n",
    "        y = np.array([is_dup], dtype=np.float32)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (6, 256, 256)\n",
    "conv_layers = (16, 32, 64, 128, 256)\n",
    "fc_layers = (128,)\n",
    "output_size = 1\n",
    "\n",
    "model_basename = 'dup_model'\n",
    "datetime_now = get_datetime_now()\n",
    "\n",
    "# Parameters\n",
    "split = 90\n",
    "batch_size = 256\n",
    "max_epochs = 30\n",
    "num_workers = 18\n",
    "learning_rate = 0.0001\n",
    "best_loss = 9999.0\n",
    "max_datapoints = 100*2**14\n",
    "\n",
    "loader_params = {\n",
    "    'train': {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_workers},\n",
    "    'valid': {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_workers}}\n",
    "\n",
    "# Datasets\n",
    "full_dataset = create_dataset_from_tiles_and_truth(sdcic)\n",
    "full_dataset = create_dataset_from_truth('output/datasets')\n",
    "print(len(full_dataset))\n",
    "np.random.shuffle(full_dataset)\n",
    "trainval_dataset = full_dataset[:max_datapoints] if max_datapoints < len(full_dataset) else full_dataset\n",
    "print(len(trainval_dataset))\n",
    "n_train, n_valid = even_split(len(trainval_dataset), batch_size, split)\n",
    "print(n_train, n_valid)\n",
    "partition = {'train': trainval_dataset[:n_train], 'valid': trainval_dataset[-n_valid:]}\n",
    "\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "#         transforms.ColorJitter(),\n",
    "        RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        RandomTransformC4(with_identity=False),\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        RandomTransformC4(with_identity=False),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# image_datasets = {x: Dataset(partition[x], train_768_dir, train_256_dir, x, image_transforms[x]) for x in ['train', 'valid']}\n",
    "image_datasets = {x: Dataset(partition[x], x, image_transforms[x]) for x in ['train', 'valid']}\n",
    "\n",
    "# Generators\n",
    "generators = {x: data.DataLoader(image_datasets[x], **loader_params[x]) for x in ['train', 'valid']}\n",
    "print(len(generators['train']), len(generators['valid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DupCNN(input_shape, output_size, conv_layers, fc_layers)\n",
    "model.cuda()\n",
    "model.to(device)\n",
    "\n",
    "loss, optimizer = create_loss_and_optimizer(model, learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Training\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    model.train()\n",
    "    t = tnrange(len(generators['train']))\n",
    "    train_iterator = iter(generators['train'])\n",
    "    for i in t:\n",
    "        t.set_description(f'Epoch {epoch + 1:>02d}')\n",
    "        # Get next batch and push to GPU\n",
    "        inputs, labels = train_iterator.next()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        #Set the parameter gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        #Forward pass, backward pass, optimize\n",
    "        outputs = model(inputs)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print statistics\n",
    "        total_train_loss += train_loss.data.item()\n",
    "        y_pred = outputs > 0.5\n",
    "        y_pred = y_pred.type_as(torch.cuda.FloatTensor())\n",
    "        equality = labels == y_pred\n",
    "        total_train_acc += equality.type_as(torch.FloatTensor()).mean()\n",
    "        \n",
    "        loss_str = f'{total_train_loss/(i + 1):.6f}'\n",
    "        acc_str = f'{total_train_acc/(i + 1):.5f}'\n",
    "        t.set_postfix(loss=loss_str, acc=acc_str)\n",
    "    \n",
    "    # Validation\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    t = tnrange(len(generators['valid']))\n",
    "    valid_iterator = iter(generators['valid'])\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i in t:\n",
    "            t.set_description(f'Validation')\n",
    "            # Get next batch and push to GPU\n",
    "            inputs, labels = valid_iterator.next()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            #Forward pass\n",
    "            val_outputs = model(inputs)\n",
    "            val_loss = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss.data.item()\n",
    "\n",
    "            y_pred = val_outputs > 0.5\n",
    "            y_pred = y_pred.type_as(torch.cuda.FloatTensor())\n",
    "            equality = labels == y_pred\n",
    "            total_val_acc += equality.type_as(torch.FloatTensor()).mean()\n",
    "        \n",
    "            loss_str = f'{total_val_loss/(i + 1):.6f}'\n",
    "            acc_str = f'{total_val_acc/(i + 1):.5f}'\n",
    "            t.set_postfix(loss=loss_str, acc=acc_str)\n",
    "\n",
    "        val_loss = total_val_loss/(i + 1)\n",
    "        if val_loss < best_loss:\n",
    "            save_checkpoint(os.path.join(\"out\", f\"{model_basename}.{datetime_now}.{epoch + 1:02d}.{val_loss:.6f}.pth\"), model)\n",
    "            save_checkpoint(os.path.join(\"out\", f\"{model_basename}.{datetime_now}.last.pth\"), model)\n",
    "            save_checkpoint(os.path.join(\"out\", f\"{model_basename}.last.pth\"), model)\n",
    "#             save_checkpoint(''.join(['out/checkpoint_', acc_str, '.pth']), model)\n",
    "            best_loss = val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
