{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from tqdm import tnrange\n",
    "\n",
    "from sdcdup.utils import get_datetime_now\n",
    "from sdcdup.utils import get_project_root\n",
    "from sdcdup.utils import even_split\n",
    "from sdcdup.utils import RandomHorizontalFlip\n",
    "from sdcdup.utils import RandomTransformC4\n",
    "from sdcdup.utils import CSVLogger\n",
    "from sdcdup.utils import ReduceLROnPlateau2\n",
    "from sdcdup.utils import ImportanceSampler\n",
    "# from sdcdup.data import create_dataset_from_tiles_and_truth\n",
    "from sdcdup.data import create_dataset_from_tiles\n",
    "# from datasets import create_dataset_from_truth\n",
    "from sdcdup.data import TrainDataset as Dataset\n",
    "# from datasets import ExternalDataset as Dataset\n",
    "from sdcdup.models import save_checkpoint\n",
    "from sdcdup.models import DupCNN\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "project_root = get_project_root()\n",
    "train_image_dir = os.path.join(project_root, os.getenv('RAW_DATA_DIR'), 'train_768')\n",
    "train_tile_dir = os.path.join(project_root, os.getenv('PROCESSED_DATA_DIR'), 'train_256')\n",
    "full_dataset_filename = os.path.join(project_root, os.getenv('PROCESSED_DATA_DIR'), 'full_SDC_dataset_from_tiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "if os.path.exists(full_dataset_filename):\n",
    "    df = pd.read_csv(full_dataset_filename)\n",
    "    full_dataset = list(zip(*[df[c].values.tolist() for c in df]))\n",
    "else:\n",
    "#     full_dataset = create_dataset_from_tiles_and_truth()\n",
    "    full_dataset = create_dataset_from_tiles()\n",
    "#     full_dataset = create_dataset_from_truth()\n",
    "    df = pd.DataFrame().append(full_dataset)\n",
    "    df.to_csv(full_dataset_filename, index=False)\n",
    "print(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (6, 256, 256)\n",
    "conv_layers = (16, 32, 64, 128, 256)\n",
    "fc_layers = (128,)\n",
    "output_size = 1\n",
    "\n",
    "model_basename = 'dup_model'\n",
    "date_time = get_datetime_now()\n",
    "\n",
    "# Parameters\n",
    "trainval_split = 0.9\n",
    "sample_rate = 0.05\n",
    "batch_size = 256\n",
    "max_epochs = 200\n",
    "num_workers = 18\n",
    "learning_rate = 0.0001\n",
    "best_loss = 9999.0\n",
    "max_datapoints = 200*2**15\n",
    "print(max_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(full_dataset)\n",
    "trainval_dataset = full_dataset[:max_datapoints] if max_datapoints < len(full_dataset) else full_dataset\n",
    "print(len(trainval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_valid = even_split(len(trainval_dataset), batch_size, trainval_split)\n",
    "partition = {'train': trainval_dataset[:n_train], 'valid': trainval_dataset[-n_valid:]}\n",
    "n_samples = batch_size * (int(round(n_train * sample_rate)) // batch_size)\n",
    "print(n_train, n_valid, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['img_id'] = pd.Series(partition['train'])\n",
    "df.to_csv(os.path.join(project_root, 'models', f'{model_basename}.{date_time}.avl.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ImportanceSampler(n_train, n_samples, batch_size)\n",
    "\n",
    "loader_params = {\n",
    "    'train': {'batch_sampler': sampler, 'num_workers': num_workers},\n",
    "#     'train': {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_workers},\n",
    "    'valid': {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_workers}}\n",
    "\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "#         RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        RandomTransformC4(with_identity=False),\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        RandomHorizontalFlip(p=1),\n",
    "        transforms.ToTensor(),\n",
    "#         RandomTransformC4(with_identity=False),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: Dataset(partition[x], x, image_transforms[x]) for x in ['train', 'valid']}\n",
    "\n",
    "# Generators\n",
    "generators = {x: data.DataLoader(image_datasets[x], **loader_params[x]) for x in ['train', 'valid']}\n",
    "print(len(generators['train']), len(generators['valid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DupCNN(input_shape, output_size, conv_layers, fc_layers)\n",
    "model.cuda()\n",
    "model.to(device)\n",
    "\n",
    "# loss = torch.nn.MSELoss()\n",
    "loss = torch.nn.BCELoss()\n",
    "# loss = torch.nn.BCEWithLogitsLoss()\n",
    "sample_loss = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = ReduceLROnPlateau2(optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"epoch\", \"time\", \"lr\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"train_time\", \"val_time\"]\n",
    "Stats = namedtuple('Stats', header)\n",
    "csv_filename = os.path.join(project_root, 'models', f'{model_basename}.{date_time}.metrics.csv')\n",
    "\n",
    "logger = CSVLogger(csv_filename, header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "#     scheduler.step()\n",
    "    \n",
    "    # Training\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    model.train()\n",
    "    t = tnrange(len(generators['train']))\n",
    "    train_iterator = iter(generators['train'])\n",
    "    for i in t:\n",
    "        t.set_description(f'Epoch {epoch + 1:>02d}')\n",
    "        # Get next batch and push to GPU\n",
    "        inputs, labels = train_iterator.next()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        train_loss = loss(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print statistics\n",
    "        sampler.update(sample_loss(outputs, labels).data.cpu().numpy())\n",
    "        total_train_loss += train_loss.data.item()\n",
    "        y_pred = outputs > 0.5\n",
    "        y_pred = y_pred.type_as(torch.cuda.FloatTensor())\n",
    "        equality = labels == y_pred\n",
    "        total_train_acc += equality.type_as(torch.FloatTensor()).numpy().mean()\n",
    "        \n",
    "        loss_str = f'{total_train_loss/(i + 1):.6f}'\n",
    "        acc_str = f'{total_train_acc/(i + 1):.5f}'\n",
    "        t.set_postfix(loss=loss_str, acc=acc_str)\n",
    "    \n",
    "    train_loss = total_train_loss/(i + 1)\n",
    "    train_acc = total_train_acc/(i + 1)\n",
    "    train_time = time.time() - t0\n",
    "    \n",
    "    # Validation\n",
    "    t1 = time.time()\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    t = tnrange(len(generators['valid']))\n",
    "    valid_iterator = iter(generators['valid'])\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i in t:\n",
    "            t.set_description(f'Validation')\n",
    "            # Get next batch and push to GPU\n",
    "            inputs, labels = valid_iterator.next()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            #Forward pass\n",
    "            val_outputs = model(inputs)\n",
    "            val_loss = loss(val_outputs, labels)\n",
    "            \n",
    "            total_val_loss += val_loss.data.item()\n",
    "            y_pred = val_outputs > 0.5\n",
    "            y_pred = y_pred.type_as(torch.cuda.FloatTensor())\n",
    "            equality = labels == y_pred\n",
    "            total_val_acc += equality.type_as(torch.FloatTensor()).numpy().mean()\n",
    "        \n",
    "            loss_str = f'{total_val_loss/(i + 1):.6f}'\n",
    "            acc_str = f'{total_val_acc/(i + 1):.5f}'\n",
    "            t.set_postfix(loss=loss_str, acc=acc_str)\n",
    "\n",
    "    val_loss = total_val_loss/(i + 1)\n",
    "    val_acc = total_val_acc/(i + 1)\n",
    "    val_time = time.time() - t1\n",
    "    \n",
    "    sampler.on_epoch_end()\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['ages'] = pd.Series(sampler.ages)\n",
    "    df['visits'] = pd.Series(sampler.visits)\n",
    "    df['losses'] = pd.Series(sampler.losses)\n",
    "    df.to_csv(os.path.join(project_root, 'models', f'{model_basename}.{date_time}.{epoch + 1:02d}.{val_loss:.6f}.avl.csv'), index=False)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        save_checkpoint(os.path.join(project_root, 'models', f'{model_basename}.{date_time}.{epoch + 1:02d}.{val_loss:.6f}.pth'), model)\n",
    "        save_checkpoint(os.path.join(project_root, 'models', f'{model_basename}.{date_time}.best.pth'), model)\n",
    "        save_checkpoint(os.path.join(project_root, 'models', f'{model_basename}.best.pth'), model)\n",
    "        best_loss = val_loss\n",
    "    \n",
    "    stats = Stats(epoch+1, total_time, scheduler.get_lr()[0], train_loss, train_acc, val_loss, val_acc, train_time, val_time)\n",
    "    logger.on_epoch_end(stats)\n",
    "    \n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Immediate post-processing section here in case we want to analyze model before shutting down notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from sdcdup.utils import fuzzy_diff\n",
    "from sdcdup.utils import fuzzy_join\n",
    "from sdcdup.utils import get_hamming_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = Counter(sampler.visits)\n",
    "\n",
    "np.min(sampler.losses), np.max(sampler.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ages = sampler.ages / np.sum(sampler.ages)\n",
    "norm_losses = sampler.losses / np.sum(sampler.losses)\n",
    "log_ages = np.log(sampler.ages)\n",
    "scaled_ages = log_ages * (np.sum(sampler.losses) / np.sum(log_ages))\n",
    "weights = scaled_ages + sampler.losses\n",
    "norm_weights = weights / np.sum(weights)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['ages'] = pd.Series(sampler.ages, dtype=int)\n",
    "df['visits'] = pd.Series(sampler.visits, dtype=int)\n",
    "df['losses'] = pd.Series(sampler.losses)\n",
    "df['norm_ages'] = pd.Series(norm_ages)\n",
    "df['norm_losses'] = pd.Series(norm_losses)\n",
    "df['log_ages'] = pd.Series(log_ages)\n",
    "df['scaled_ages'] = pd.Series(scaled_ages)\n",
    "df['weights'] = pd.Series(weights)\n",
    "df['norm_weights'] = pd.Series(norm_weights)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['losses', 'scaled_ages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_loss_indices = np.where(sampler.losses > 0.16)[0]\n",
    "sampler.losses[bad_loss_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_overlaps = []\n",
    "for i in np.where(sampler.losses > 0.16)[0]:\n",
    "    bad_overlaps.append(partition['train'][i])\n",
    "len(bad_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in bad_loss_indices:\n",
    "    bol = partition['train'][i]\n",
    "    \n",
    "    bmh1 = sdcic.img_metrics['bmh'][bol[0]][bol[2]]\n",
    "    bmh2 = sdcic.img_metrics['bmh'][bol[1]][bol[3]]\n",
    "    score = get_hamming_distance(bmh1, bmh2, as_score=True)\n",
    "\n",
    "    tile1 = sdcic.get_tile(sdcic.get_img(bol[0]), bol[2])\n",
    "    tile2 = sdcic.get_tile(sdcic.get_img(bol[1]), bol[3])\n",
    "    tile3 = fuzzy_join(tile1, tile2)\n",
    "    pix3, cts3 = np.unique(tile3.flatten(), return_counts=True)\n",
    "\n",
    "    print(bol, f'{np.max(cts3 / (256*256*3)):>.4f}', f'{sampler.losses[i]:>.6f}', np.sum(tile1 != tile2), fuzzy_diff(tile1, tile2))\n",
    "    for chan in range(3):\n",
    "        pix1, cts1 = np.unique(tile1[:, :, chan].flatten(), return_counts=True)\n",
    "        pix2, cts2 = np.unique(tile2[:, :, chan].flatten(), return_counts=True)\n",
    "        pix3, cts3 = np.unique(tile3[:, :, chan].flatten(), return_counts=True)\n",
    "\n",
    "        max_idx1 = np.argmax(cts1)\n",
    "        max_pix1 = pix1[max_idx1]\n",
    "        max_cts1 = cts1[max_idx1]\n",
    "#         print(f'{max_pix1:>3}', max_cts1, f'{max_cts1/65536:.4f}')\n",
    "        max_idx2 = np.argmax(cts2)\n",
    "        max_pix2 = pix2[max_idx2]\n",
    "        max_cts2 = cts2[max_idx2]\n",
    "#         print(f'{max_pix2:>3}', max_cts2, f'{max_cts2/65536:.4f}')\n",
    "        max_idx3 = np.argmax(cts3)\n",
    "        max_pix3 = pix3[max_idx3]\n",
    "        max_cts3 = cts3[max_idx3]\n",
    "#         print(f'{max_pix3:>3}', max_cts3, f'{max_cts3/65536:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 0\n",
    "for bol in sorted(full_dataset):\n",
    "    bmh1 = sdcic.img_metrics['bmh'][bol[0]][bol[2]]\n",
    "    bmh2 = sdcic.img_metrics['bmh'][bol[1]][bol[3]]\n",
    "    score = get_hamming_distance(bmh1, bmh2, as_score=True)\n",
    "\n",
    "    if not (score == 256 and bol[4] == 0):\n",
    "        continue\n",
    "        \n",
    "    tile1 = sdcic.get_tile(sdcic.get_img(bol[0]), bol[2])\n",
    "    tile2 = sdcic.get_tile(sdcic.get_img(bol[1]), bol[3])\n",
    "    tile3 = fuzzy_join(tile1, tile2)\n",
    "    pix3, cts3 = np.unique(tile3.flatten(), return_counts=True)\n",
    "    \n",
    "    if np.max(cts3 / (256*256*3)) > 0.97:\n",
    "        ii += 1\n",
    "        print(ii, bol, f'{np.max(cts3 / (256*256*3)):>.4f}', np.sum(tile1 != tile2), fuzzy_diff(tile1, tile2))\n",
    "    \n",
    "    continue\n",
    "    \n",
    "    for chan in range(3):\n",
    "        pix1, cts1 = np.unique(tile1[:, :, chan].flatten(), return_counts=True)\n",
    "        pix2, cts2 = np.unique(tile2[:, :, chan].flatten(), return_counts=True)\n",
    "        pix3, cts3 = np.unique(tile3[:, :, chan].flatten(), return_counts=True)\n",
    "\n",
    "        max_idx1 = np.argmax(cts1)\n",
    "        max_idx2 = np.argmax(cts2)\n",
    "        max_idx3 = np.argmax(cts3)\n",
    "\n",
    "        max_pix1 = pix1[max_idx1]\n",
    "        max_pix2 = pix2[max_idx2]\n",
    "        max_pix3 = pix3[max_idx3]\n",
    "        \n",
    "        max_cts1 = cts1[max_idx1]\n",
    "        max_cts2 = cts2[max_idx2]\n",
    "        max_cts3 = cts3[max_idx3]\n",
    "        \n",
    "        if min([max_cts1, max_cts2, max_cts3])/65536 >= 0.95:\n",
    "            continue\n",
    "        \n",
    "        ii += 1\n",
    "        print(ii, bol)\n",
    "        print(f'{max_pix1:>3}', max_cts1, f'{max_cts1/65536:.4f}')\n",
    "        print(f'{max_pix2:>3}', max_cts2, f'{max_cts2/65536:.4f}')\n",
    "        print(f'{max_pix3:>3}', max_cts3, f'{max_cts3/65536:.4f}')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
