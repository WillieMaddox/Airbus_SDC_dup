{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sdcdup.utils import get_project_root\n",
    "from sdcdup.utils import load_duplicate_truth\n",
    "from sdcdup.features import load_image_overlap_properties\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "project_root = get_project_root()\n",
    "train_image_dir = os.path.join(project_root, os.getenv('RAW_DATA_DIR'), 'train_768')\n",
    "processed_data_dir = os.path.join(project_root, os.getenv('PROCESSED_DATA_DIR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_truth = load_duplicate_truth()\n",
    "print(len(dup_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_types = ['bmh', 'cmh', 'epy', 'enp']\n",
    "overlap_image_maps = load_image_overlap_properties()\n",
    "print(len(overlap_image_maps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we explore dup detection using image gradients and cross entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_entropy(ctr, img_size=1769472):  # 768*768*3 = 1769472\n",
    "    ctr_norm = {k: v / img_size for k, v in sorted(ctr.items())}\n",
    "    ctr_entropy = {k: -v * np.log(v) for k, v in ctr_norm.items()}\n",
    "    entropy = np.sum([k * v for k, v in ctr_entropy.items()])\n",
    "    return entropy\n",
    "\n",
    "def get_entropy(img_id):\n",
    "    img = cv2.imread(os.path.join(train_image_dir, img_id))\n",
    "    img_grad = np.gradient(img.astype(np.int), axis=(0, 1))\n",
    "    entropy_list = []\n",
    "    for channel_grad in img_grad:\n",
    "        ctr = Counter(np.abs(channel_grad).flatten())\n",
    "        entropy_list.append(get_channel_entropy(ctr, img.size))\n",
    "    return np.array(entropy_list)\n",
    "\n",
    "def get_entropy1(img_id):\n",
    "    img = cv2.imread(os.path.join(train_image_dir, img_id))\n",
    "    img_grad = np.gradient(img.astype(np.int), 0.5, axis=(0, 1))\n",
    "    entropy_list = []\n",
    "    for channel_grad in img_grad:\n",
    "        ctr = Counter(np.abs(channel_grad).astype(np.uint8).flatten())\n",
    "        entropy_list.append(ctr)\n",
    "    return entropy_list\n",
    "\n",
    "def get_entropy2(img1_id, img2_id):\n",
    "    entropy1_list = get_entropy1(img1_id)\n",
    "    entropy2_list = get_entropy1(img2_id)\n",
    "    entropy_list = []\n",
    "    for ctr1, ctr2 in zip(entropy1_list, entropy2_list):\n",
    "        ctr = (ctr1 - ctr2) + (ctr2 - ctr1)\n",
    "        entropy_list.append(get_channel_entropy(ctr))\n",
    "    return np.array(entropy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_lim0 = 0\n",
    "score_lim1 = 1\n",
    "for (img1_id, img2_id), overlap_maps in tqdm_notebook(overlap_image_maps.items()):\n",
    "    if img1_id > img2_id:\n",
    "        # sanity check\n",
    "        raise ValueError(f'img1_id ({img1_id}) should be lexicographically smaller than img2_id ({img2_id})')\n",
    "    for img1_overlap_tag, scores in overlap_maps.items():\n",
    "        if (img1_id, img2_id, img1_overlap_tag) not in dup_truth:\n",
    "            continue\n",
    "        \n",
    "        is_dup = dup_truth[(img1_id, img2_id, img1_overlap_tag)]\n",
    "\n",
    "        if is_dup == 0 and np.max(scores.enp) > score_lim0:\n",
    "            score_lim0 = np.max(scores.enp)\n",
    "            print_score = True\n",
    "        elif is_dup == 1 and np.max(scores.enp) < score_lim1:\n",
    "            score_lim1 = np.max(scores.enp)\n",
    "            print_score = True\n",
    "        else:\n",
    "            print_score = False\n",
    "\n",
    "        if print_score:\n",
    "            img1_entropy_vec = get_entropy(img1_id)\n",
    "            img2_entropy_vec = get_entropy(img2_id)\n",
    "            img1_entropy_vec_norm = np.linalg.norm(img1_entropy_vec)\n",
    "            img2_entropy_vec_norm = np.linalg.norm(img2_entropy_vec)\n",
    "            n_vec = np.max([img1_entropy_vec_norm, img2_entropy_vec_norm])\n",
    "            img1_scaled_vec = img1_entropy_vec / n_vec\n",
    "            img2_scaled_vec = img2_entropy_vec / n_vec\n",
    "            grad_score = 1.0 - np.linalg.norm(img1_scaled_vec - img2_scaled_vec)\n",
    "\n",
    "            entropy2 = get_entropy2(img1_id, img2_id)\n",
    "            entropy2_norm = np.linalg.norm(entropy2)\n",
    "            \n",
    "            print('')\n",
    "            print(f'{is_dup}, {min(scores.bmh):7.5f}, {min(scores.cmh):7.5f}, {grad_score:7.5f}, {entropy2_norm}')\n",
    "            print(img1_id, img1_entropy_vec, f'{img1_entropy_vec_norm}')\n",
    "            print(img2_id, img2_entropy_vec, f'{img2_entropy_vec_norm}')\n",
    "            print(get_entropy(img1_id))\n",
    "            print(get_entropy(img2_id))\n",
    "            print(entropy2)\n",
    "            print(np.max(scores.enp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_id = '691d5afc2.jpg'\n",
    "img2_id = '56417e7af.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_entropy_vec = get_entropy(img1_id)\n",
    "img2_entropy_vec = get_entropy(img2_id)\n",
    "img1_entropy_vec_norm = np.linalg.norm(img1_entropy_vec)\n",
    "img2_entropy_vec_norm = np.linalg.norm(img2_entropy_vec)\n",
    "n_vec = np.max([img1_entropy_vec_norm, img1_entropy_vec_norm])\n",
    "img1_scaled_vec = img1_entropy_vec / n_vec\n",
    "img2_scaled_vec = img2_entropy_vec / n_vec\n",
    "print('')\n",
    "print(img1_id, img1_entropy_vec, f'{img1_entropy_vec_norm}')\n",
    "print(img2_id, img2_entropy_vec, f'{img1_entropy_vec_norm}')\n",
    "print(f'{np.linalg.norm(img1_scaled_vec - img2_scaled_vec)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(processed_data_dir, 'dup_blacklist_6.csv'), sep=', ')\n",
    "for idx, row in df.iterrows():\n",
    "    print(idx)\n",
    "    img1_entropy_vec = get_entropy(row['ImageId1'])\n",
    "    img1_entropy_vec_u = img1_entropy_vec / np.linalg.norm(img1_entropy_vec)\n",
    "    print(row['ImageId1'], img1_entropy_vec)\n",
    "    img2_entropy_vec = get_entropy(row['ImageId2'])\n",
    "    img2_entropy_vec_u = img2_entropy_vec / np.linalg.norm(img2_entropy_vec)\n",
    "    print(row['ImageId2'], img2_entropy_vec)\n",
    "    print(np.dot(img1_entropy_vec_u, img2_entropy_vec_u), np.linalg.norm(img1_entropy_vec - img2_entropy_vec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
