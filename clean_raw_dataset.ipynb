{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import operator\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage.util import montage\n",
    "import cv2\n",
    "from cv2 import img_hash\n",
    "import torch\n",
    "\n",
    "from utils import overlay_tag_pairs\n",
    "from utils import overlay_tag_maps\n",
    "from utils import generate_overlay_tag_slices\n",
    "from utils import get_tile\n",
    "from utils import get_overlay_score\n",
    "from utils import get_tile_scores\n",
    "from utils import get_entropy_score\n",
    "from utils import gen_pixel_scores\n",
    "from utils import channel_shift\n",
    "from utils import read_duplicate_truth\n",
    "from utils import update_duplicate_truth\n",
    "from utils import read_image_duplicate_tiles\n",
    "from utils import write_image_duplicate_tiles\n",
    "from utils import read_image_image_duplicate_tiles\n",
    "from utils import update_image_image_duplicate_tiles\n",
    "from utils import generate_overlay_tag_nines_mask\n",
    "\n",
    "from test_friend_circles import SDCImageContainer\n",
    "\n",
    "from dupnet import load_checkpoint\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "EPS = np.finfo(np.float32).eps\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 16\n",
    "BIGGEST_SIZE = 20\n",
    "plt.rc('font', size=BIGGEST_SIZE)         # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGEST_SIZE)    # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGEST_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=BIGGER_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=BIGGER_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)   # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGEST_SIZE)  # fontsize of the figure title\n",
    "\n",
    "montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n",
    "montage_pad = lambda x, *args, **kwargs: montage(x, padding_width=10, *args, **kwargs)\n",
    "\n",
    "ship_dir = \"data/input\"\n",
    "train_image_dir = os.path.join(ship_dir, \"train_768\")\n",
    "train_mask_dir = os.path.join(ship_dir, 'train_masks_768')\n",
    "image_counter_grids_file = os.path.join(\"data\", \"image_counter_grids.pkl\")\n",
    "image_md5hash_grids_file = os.path.join(\"data\", \"image_md5hash_grids.pkl\")\n",
    "image_bm0hash_grids_file = os.path.join(\"data\", \"image_bm0hash_grids.pkl\")\n",
    "image_entropy_grids_file = os.path.join(\"data\", \"image_entropy_grids.pkl\")\n",
    "image_duplicate_tiles_file = os.path.join(\"data\", \"image_duplicate_tiles.txt\")\n",
    "image_image_duplicate_tiles_file = os.path.join(\"data\", \"image_image_duplicate_tiles.txt\")\n",
    "duplicate_truth_file = os.path.join('data', 'duplicate_truth.txt')\n",
    "\n",
    "overlay_tag_slices = generate_overlay_tag_slices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_entropy(ctr, img_size=1769472):  # 768x768x3\n",
    "    ctr_norm = {k: v / img_size for k, v in sorted(ctr.items())}\n",
    "    ctr_entropy = {k: -v * np.log(v) for k, v in ctr_norm.items()}\n",
    "    entropy = np.sum([k * v for k, v in ctr_entropy.items()])\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def get_entropy(img_id):\n",
    "    img = cv2.imread(os.path.join(train_image_dir, img_id))\n",
    "    img_grad = np.gradient(img.astype(np.int), axis=(0, 1))\n",
    "    entropy_list = []\n",
    "    for channel_grad in img_grad:\n",
    "        ctr = Counter(np.abs(channel_grad).flatten())\n",
    "        entropy_list.append(get_channel_entropy(ctr, img.size))\n",
    "    return np.array(entropy_list)\n",
    "\n",
    "def get_entropy1(img_id):\n",
    "    img = cv2.imread(os.path.join(train_image_dir, img_id))\n",
    "    img_grad = np.gradient(img.astype(np.int), 0.5, axis=(0, 1))\n",
    "    entropy_list = []\n",
    "    for channel_grad in img_grad:\n",
    "        ctr = Counter(np.abs(channel_grad).astype(np.uint8).flatten())\n",
    "        entropy_list.append(ctr)\n",
    "    return entropy_list\n",
    "\n",
    "def get_entropy2(img1_id, img2_id):\n",
    "    entropy1_list = get_entropy1(img1_id)\n",
    "    entropy2_list = get_entropy1(img2_id)\n",
    "    entropy_list = []\n",
    "    for ctr1, ctr2 in zip(entropy1_list, entropy2_list):\n",
    "        ctr = (ctr1 - ctr2) + (ctr2 - ctr1)\n",
    "        entropy_list.append(get_channel_entropy(ctr))\n",
    "    return np.array(entropy_list)\n",
    "\n",
    "def slice_from_large(img, idx, sz=256):\n",
    "    tile = get_tile(img, idx, sz=sz)\n",
    "    return cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def gen_dcnn_scores(img1, img2, img1_overlay_tag, model, sz=256):    \n",
    "    img1_overlay_map = overlay_tag_maps[img1_overlay_tag]\n",
    "    img2_overlay_map = overlay_tag_maps[overlay_tag_pairs[img1_overlay_tag]]\n",
    "    X_list = []\n",
    "    for idx1, idx2 in zip(img1_overlay_map, img2_overlay_map):\n",
    "        tile1 = slice_from_large(img1, idx1, sz=sz).astype(np.float32) / 255.0\n",
    "        tile2 = slice_from_large(img2, idx2, sz=sz).astype(np.float32) / 255.0\n",
    "        X = np.dstack([tile1, tile2])\n",
    "        X = X.transpose((2, 0, 1))\n",
    "        X_list.append(X)\n",
    "\n",
    "    X_arr = np.stack(X_list)\n",
    "    inputs = torch.from_numpy(X_arr)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        val_outputs = model(inputs)\n",
    "\n",
    "    return val_outputs[:, 0].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgMod:\n",
    "    \"\"\"\n",
    "    Reads a single image to be modified by hls.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.img_id = filename.split('/')[-1]\n",
    "\n",
    "        self._hls_chan = None\n",
    "        self._hls_gain = None\n",
    "\n",
    "        self._parent_bgr = None\n",
    "        self._parent_hls = None\n",
    "        self._parent_rgb = None\n",
    "        self._cv2_hls = None\n",
    "        self._cv2_bgr = None\n",
    "        self._cv2_rgb = None\n",
    "\n",
    "    def channel_shift(self, chan, gain):\n",
    "        self._hls_chan = chan\n",
    "        self._hls_gain = gain\n",
    "        self._cv2_hls = None\n",
    "        return self.cv2_rgb\n",
    "    \n",
    "    def scale(self, minval, maxval):\n",
    "        m = 255.0 * (maxval - minval)\n",
    "        res = m * (self.parent_bgr - minval)\n",
    "        return np.around(res).astype(np.uint8)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.parent_bgr.shape\n",
    "    \n",
    "    @property\n",
    "    def parent_bgr(self):\n",
    "        if self._parent_bgr is None:\n",
    "            self._parent_bgr = cv2.imread(self.filename)\n",
    "        return self._parent_bgr\n",
    "\n",
    "    @property\n",
    "    def parent_hls(self):\n",
    "        if self._parent_hls is None:\n",
    "            self._parent_hls = self.to_hls(self.parent_bgr)\n",
    "        return self._parent_hls\n",
    "\n",
    "    @property\n",
    "    def parent_rgb(self):\n",
    "        if self._parent_rgb is None:\n",
    "            self._parent_rgb = self.to_rgb(self.parent_bgr)\n",
    "        return self._parent_rgb\n",
    "\n",
    "    @property\n",
    "    def cv2_hls(self):\n",
    "        if self._cv2_hls is None:\n",
    "            if self._hls_gain == None:\n",
    "                self._cv2_hls = self.parent_hls\n",
    "            else:\n",
    "                self._cv2_hls = channel_shift(self.parent_hls, self._hls_chan, self._hls_gain)\n",
    "        return self._cv2_hls\n",
    "\n",
    "    @property\n",
    "    def cv2_bgr(self):\n",
    "        if self._cv2_bgr is None:\n",
    "            self._cv2_bgr = self.to_bgr(self.cv2_hls)\n",
    "        return self._cv2_bgr\n",
    "\n",
    "    @property\n",
    "    def cv2_rgb(self):\n",
    "        if self._cv2_rgb is None:\n",
    "            self._cv2_rgb = self.to_rgb(self.cv2_bgr)\n",
    "        return self._cv2_rgb\n",
    "\n",
    "    def to_hls(self, bgr):\n",
    "        return cv2.cvtColor(bgr, cv2.COLOR_BGR2HLS_FULL)\n",
    "\n",
    "    def to_bgr(self, hls):\n",
    "        return cv2.cvtColor(hls, cv2.COLOR_HLS2BGR_FULL)\n",
    "\n",
    "    def to_rgb(self, bgr):\n",
    "        return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdcic = SDCImageContainer(train_image_dir)\n",
    "sdcic.load_3x3_grids(\n",
    "    image_counter_grids_file,\n",
    "    image_md5hash_grids_file,\n",
    "    image_bm0hash_grids_file,\n",
    "    image_entropy_grids_file,\n",
    "    image_duplicate_tiles_file)\n",
    "\n",
    "dup_truth = read_duplicate_truth(duplicate_truth_file)\n",
    "len(dup_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_matching_tiles = 6\n",
    "overlay_matches_file = os.path.join(\"data\", f\"overlay_matches_{n_matching_tiles}.pkl\")\n",
    "overlay_pixel_scores_file = os.path.join(\"data\", f\"overlay_pixel_scores_{n_matching_tiles}.pkl\")\n",
    "\n",
    "matches = defaultdict(list)\n",
    "df = pd.read_pickle(overlay_matches_file)\n",
    "for row in tqdm_notebook(df.to_dict('split')['data']):\n",
    "    matches[(row[0], row[1])].append((row[2], row[3], row[4:]))\n",
    "\n",
    "overlay_pixel_scores = {}\n",
    "df = pd.read_pickle(overlay_pixel_scores_file)\n",
    "for row in tqdm_notebook(df.to_dict('split')['data']):\n",
    "    if (row[0], row[1]) not in overlay_pixel_scores:\n",
    "        overlay_pixel_scores[(row[0], row[1])] = {}\n",
    "    assert row[2] not in overlay_pixel_scores[(row[0], row[1])]\n",
    "    overlay_pixel_scores[(row[0], row[1])][row[2]] = row[3:]\n",
    "\n",
    "for (img1_id, img2_id), values in tqdm_notebook(sorted(matches.items())):\n",
    "    img1_overlay_tags = set([v[0] for v in values])  # all have the same overlay_tag\n",
    "    if len(values) == 0 or len(img1_overlay_tags) != 1:\n",
    "        continue\n",
    "    img1_overlay_tag, overlay_score, tile_scores = values[0]\n",
    "    pixel_scores = overlay_pixel_scores[(img1_id, img2_id)][img1_overlay_tag]\n",
    "    entropy_score = get_entropy_score(img1_id, img2_id, img1_overlay_tag, sdcic.tile_entropy_grids)\n",
    "    sdcic.update_overlay_maps(img1_id, img2_id, img1_overlay_tag, overlay_score, tile_scores, pixel_scores, entropy_score)\n",
    "\n",
    "print(len(sdcic.overlay_image_maps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find overlapping images using hashlib\n",
    "Update: The values between two supposedly exact 256x256 crops are not exact (See below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md5hash_dict = defaultdict(list)\n",
    "img_ids = os.listdir(train_image_dir)\n",
    "\n",
    "for img_id in tqdm_notebook(img_ids):\n",
    "    for h in sdcic.tile_md5hash_grids[img_id]:\n",
    "        md5hash_dict[h].append(img_id)\n",
    "\n",
    "dup_counts_dict = defaultdict(int)\n",
    "for key, dups in md5hash_dict.items():\n",
    "    dup_counts_dict[len(dups)] += 1\n",
    "\n",
    "sorted_bin_sizes = sorted(dup_counts_dict.items())\n",
    "sorted_bin_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 9\n",
    "skip = 365\n",
    "ii = 0\n",
    "jj = 0\n",
    "batch_limit = 9\n",
    "samples_images = np.empty((batch_limit, 768, 768, 3), dtype=np.float32)\n",
    "\n",
    "for hash_id, dups in md5hash_dict.items():\n",
    "    ii += 1\n",
    "    if len(dups) == batch_size:\n",
    "        dups0 = list(set(dups))\n",
    "        img_id = dups0[0]\n",
    "        idx = sdcic.tile_md5hash_grids[img_id].index(hash_id)\n",
    "        print(hash_id, len(dups), ii, sdcic.tile_entropy_grids[img_id][idx])\n",
    "        if jj == min(dup_counts_dict[len(dups)], skip):\n",
    "            break\n",
    "        jj += 1\n",
    "\n",
    "for i, c_img_id in enumerate(dups0[:batch_limit]):\n",
    "    c_img = cv2.cvtColor(sdcic.get_img(c_img_id), cv2.COLOR_BGR2RGB)\n",
    "    samples_images[i] = c_img.astype(np.float32) / 255.0\n",
    "\n",
    "batch_rgb = montage_rgb(samples_images)\n",
    "print(samples_images.shape)\n",
    "print(batch_rgb.shape, batch_rgb.dtype)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (16, 16))\n",
    "ax.imshow(batch_rgb, vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "# plt.savefig(os.path.join('out', BASE_MODEL, f\"{train_meta_filebase}_{score_str}_batch_{BATCH_NUM}.jpg\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find overlapping images using cv2.img_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm0hash_dict = defaultdict(list)\n",
    "img_ids = os.listdir(train_image_dir)\n",
    "\n",
    "for img_id in tqdm_notebook(img_ids):\n",
    "    for h in sdcic.tile_bm0hash_grids[img_id]:\n",
    "        bm0hash_dict[tuple(h)].append(img_id)  # hex\n",
    "\n",
    "dup_counts_dict = defaultdict(int)\n",
    "for key, dups in bm0hash_dict.items():\n",
    "    dup_counts_dict[len(dups)] += 1\n",
    "\n",
    "sorted_bin_sizes = sorted(dup_counts_dict.items())\n",
    "sorted_bin_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 18\n",
    "skip = 5\n",
    "ii = 0\n",
    "jj = 0\n",
    "batch_limit = 9\n",
    "samples_images = np.empty((batch_limit, 768, 768, 3), dtype=np.float32)\n",
    "\n",
    "for hash_id, dups in bm0hash_dict.items():\n",
    "    ii += 1\n",
    "    if len(dups) == batch_size:\n",
    "        dups0 = list(set(dups))\n",
    "        img_id = dups0[0]\n",
    "        idx = sdcic.tile_bm0hash_grids[img_id].index(hash_id)\n",
    "        print(hash_id, len(dups), ii, sdcic.tile_entropy_grids[img_id][idx])\n",
    "        if jj == min(dup_counts_dict[len(dups)], skip):\n",
    "            break\n",
    "        jj += 1\n",
    "\n",
    "for i, c_img_id in enumerate(dups0[:batch_limit]):\n",
    "    c_img = cv2.cvtColor(sdcic.get_img(c_img_id), cv2.COLOR_BGR2RGB)\n",
    "    samples_images[i] = c_img.astype(np.float32) / 255.0\n",
    "\n",
    "batch_rgb = montage_rgb(samples_images)\n",
    "print(samples_images.shape)\n",
    "print(batch_rgb.shape, batch_rgb.dtype)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (16, 16))\n",
    "ax.imshow(batch_rgb, vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "# plt.savefig(os.path.join('out', BASE_MODEL, f\"{train_meta_filebase}_{score_str}_batch_{BATCH_NUM}.jpg\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we explore dup detection using image gradients and cross entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_lim0 = 0\n",
    "score_lim1 = 1\n",
    "for (img1_id, img2_id), overlay_maps in tqdm_notebook(sdcic.overlay_image_maps.items()):\n",
    "    if img1_id > img2_id:\n",
    "        # sanity check\n",
    "        raise ValueError(f'img1_id ({img1_id}) should be lexicographically smaller than img2_id ({img2_id})')\n",
    "    for img1_overlay_tag, (overlay_score, tile_scores, pixel_scores, entropy_score) in overlay_maps.items():\n",
    "        if (img1_id, img2_id, img1_overlay_tag) not in dup_truth:\n",
    "            continue\n",
    "        is_dup = dup_truth[(img1_id, img2_id, img1_overlay_tag)]\n",
    "\n",
    "        if is_dup == 0 and entropy_score > score_lim0:\n",
    "            score_lim0 = entropy_score\n",
    "            print_score = True\n",
    "        elif is_dup == 1 and entropy_score < score_lim1:\n",
    "            score_lim1 = entropy_score\n",
    "            print_score = True\n",
    "        else:\n",
    "            print_score = False\n",
    "\n",
    "        if print_score:\n",
    "            img1_entropy_vec = get_entropy(img1_id)\n",
    "            img2_entropy_vec = get_entropy(img2_id)\n",
    "            img1_entropy_vec_norm = np.linalg.norm(img1_entropy_vec)\n",
    "            img2_entropy_vec_norm = np.linalg.norm(img2_entropy_vec)\n",
    "            n_vec = np.max([img1_entropy_vec_norm, img2_entropy_vec_norm])\n",
    "            img1_scaled_vec = img1_entropy_vec / n_vec\n",
    "            img2_scaled_vec = img2_entropy_vec / n_vec\n",
    "            grad_score = 1.0 - np.linalg.norm(img1_scaled_vec - img2_scaled_vec)\n",
    "\n",
    "            entropy2 = get_entropy2(img1_id, img2_id)\n",
    "            entropy2_norm = np.linalg.norm(entropy2)\n",
    "            \n",
    "            print('')\n",
    "            print(f\"{is_dup}, {overlay_score:7.5f}, {min(tile_scores):7.5f}, {grad_score:7.5f}, {entropy2_norm}\")\n",
    "            print(img1_id, img1_entropy_vec, f\"{img1_entropy_vec_norm}\")\n",
    "            print(img2_id, img2_entropy_vec, f\"{img2_entropy_vec_norm}\")\n",
    "            print(get_entropy(img1_id))\n",
    "            print(get_entropy(img2_id))\n",
    "            print(entropy2)\n",
    "            print(entropy_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_id = '691d5afc2.jpg'\n",
    "img2_id = '56417e7af.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread(os.path.join(train_image_dir, img2_id))\n",
    "img2_grad = np.gradient(img2.astype(np.int), 0.5, axis=(0, 1))\n",
    "ctr2 = Counter(np.abs(img2_grad[0]).astype(np.uint8).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_entropy_vec = get_entropy(img1_id)\n",
    "img2_entropy_vec = get_entropy(img2_id)\n",
    "img1_entropy_vec_norm = np.linalg.norm(img1_entropy_vec)\n",
    "img2_entropy_vec_norm = np.linalg.norm(img2_entropy_vec)\n",
    "n_vec = np.max([img1_entropy_vec_norm, img1_entropy_vec_norm])\n",
    "img1_scaled_vec = img1_entropy_vec / n_vec\n",
    "img2_scaled_vec = img2_entropy_vec / n_vec\n",
    "print('')\n",
    "print(img1_id, img1_entropy_vec, f\"{img1_entropy_vec_norm}\")\n",
    "print(img2_id, img2_entropy_vec, f\"{img1_entropy_vec_norm}\")\n",
    "print(f\"{np.linalg.norm(img1_scaled_vec - img2_scaled_vec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dup_blacklist_6.csv', sep=', ')\n",
    "for idx, row in df.iterrows():\n",
    "    print(idx)\n",
    "    img1_entropy_vec = get_entropy(row['ImageId1'])\n",
    "    img1_entropy_vec_u = img1_entropy_vec / np.linalg.norm(img1_entropy_vec)\n",
    "    print(row['ImageId1'], img1_entropy_vec)\n",
    "    img2_entropy_vec = get_entropy(row['ImageId2'])\n",
    "    img2_entropy_vec_u = img2_entropy_vec / np.linalg.norm(img2_entropy_vec)\n",
    "    print(row['ImageId2'], img2_entropy_vec)\n",
    "    print(np.dot(img1_entropy_vec_u, img2_entropy_vec_u), np.linalg.norm(img1_entropy_vec - img2_entropy_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an interactive widget for tagging duplicate overlays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Button, Image, Layout, Box, HBox, VBox, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the various scores together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overlay_Scores = namedtuple('overlay_scores', ['overlay', 'tile', 'pixel', 'entropy'])\n",
    "overlay_candidates = []\n",
    "for (img1_id, img2_id), overlay_maps in tqdm_notebook(sdcic.overlay_image_maps.items()):\n",
    "    if img1_id > img2_id:\n",
    "        # sanity check\n",
    "        raise ValueError(f'img1_id ({img1_id}) should be lexicographically smaller than img2_id ({img2_id})')\n",
    "    for img1_overlay_tag, (overlay_score, tile_scores, pixel_scores, entropy_score) in overlay_maps.items():\n",
    "        if (img1_id, img2_id, img1_overlay_tag) in dup_truth:\n",
    "            continue\n",
    "        overlay_scores = Overlay_Scores(overlay_score, tile_scores, pixel_scores, entropy_score)\n",
    "        overlay_candidates.append((img1_id, img2_id, img1_overlay_tag, overlay_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdcic.image_image_duplicate_tiles = read_image_image_duplicate_tiles(image_image_duplicate_tiles_file)\n",
    "overlay_tag_nines_mask = generate_overlay_tag_nines_mask()\n",
    "\n",
    "duplicate_candidates = []\n",
    "for img1_id, img2_id, img1_overlay_tag, overlay_scores in sorted(overlay_candidates, key=operator.itemgetter(0, 2), reverse=False):\n",
    "#     img1_nine, img2_nine = sdcic.image_image_duplicate_tiles[(img1_id, img2_id)]\n",
    "#     img1_mask = img1_nine[overlay_tag_nines_mask[img1_overlay_tag]]\n",
    "#     img2_mask = img2_nine[overlay_tag_nines_mask[overlay_tag_pairs[img1_overlay_tag]]]\n",
    "    \n",
    "    # This is here so I don't forget to address small 2 tile or 1 tile overlays later.\n",
    "#     if len(img1_mask) <= 2:\n",
    "#         print(img1_mask, img2_mask)\n",
    "#         continue\n",
    "    \n",
    "    # (0, 3, 6) == (0, 3, 6) is exact duplicate\n",
    "#     if len(set(img1_mask)) == len(img1_mask) and np.all(img1_mask == img2_mask) and 9 not in img1_mask:\n",
    "#         continue\n",
    "    \n",
    "    # (0, 0, 0) == (0, 0, 0) skip probably is duplicate of white clouds, or blue border.\n",
    "#     if len(set(img1_mask) | set(img2_mask)) == 1 and 9 not in img1_mask:\n",
    "#         continue\n",
    "    \n",
    "    # (0, 0, 0) == (2, 2, 2) is NOT duplicate. probably white clouds overlay with blue boarder\n",
    "#     if len(set(img1_mask)) == 1 and len(set(img2_mask)) == 1 and set(img1_mask) != set(img2_mask) and 9 not in img1_mask and 9 not in img2_mask:\n",
    "#         continue\n",
    "        \n",
    "#     if len(set(img1_mask)) == 1 and len(set(img2_mask)) != 1 and 9 not in img1_mask:\n",
    "#         if set(img1_mask) != set(img2_mask):\n",
    "#         continue\n",
    "    \n",
    "#     if overlay_scores.entropy <= 0.995 or overlay_scores.entropy >= 0.999:\n",
    "#         continue\n",
    "    \n",
    "    duplicate_candidates.append((img1_id, img2_id, img1_overlay_tag, overlay_scores))\n",
    "\n",
    "print(len(overlay_candidates), len(duplicate_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_iter = iter(duplicate_candidates)\n",
    "overlay_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "img1_id = None\n",
    "img2_id = None\n",
    "img1_overlay_tag = None\n",
    "\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    align_items='stretch',\n",
    "                    width='100%')\n",
    "\n",
    "def get_next_img_pair():\n",
    "    global img1_id, img2_id, img1_overlay_tag\n",
    "    \n",
    "    while True:\n",
    "        img1_id, img2_id, img1_overlay_tag, scores = next(candidates_iter)\n",
    "        \n",
    "        if img1_id > img2_id:\n",
    "            continue\n",
    "        \n",
    "        if (img1_id, img2_id, img1_overlay_tag) in overlay_labels:\n",
    "            continue\n",
    "        \n",
    "        img1_nine, img2_nine = sdcic.image_image_duplicate_tiles[(img1_id, img2_id)]\n",
    "        img1_mask = img1_nine[overlay_tag_nines_mask[img1_overlay_tag]]\n",
    "        img2_mask = img2_nine[overlay_tag_nines_mask[overlay_tag_pairs[img1_overlay_tag]]]\n",
    "\n",
    "        if ((len(set(img1_mask)) == 1 and 9 not in img1_mask) or (len(set(img2_mask)) == 1 and 9 not in img2_mask)) and scores.entropy < 0.001:            #one of these images is all white clouds or blue border or black background\n",
    "            continue\n",
    "\n",
    "#         if len(set(img1_mask)) == 1 and len(set(img2_mask)) == 1 and set(img1_mask) == set(img2_mask) and 9 not in img1_mask and 9 not in img2_mask:\n",
    "#             overlay_labels[(img1_id, img2_id, img1_overlay_tag)] = 0\n",
    "#             continue\n",
    "\n",
    "#         if scores.entropy > 0.999:\n",
    "#             continue\n",
    "        \n",
    "        if max(scores.pixel) < 200:\n",
    "            continue\n",
    "        \n",
    "        break\n",
    "\n",
    "    return img1_id, img2_id, img1_overlay_tag, scores\n",
    "\n",
    "def draw_images(img1_id, img2_id, img1_overlay_tag, scores):\n",
    "    \n",
    "    imgmod1 = ImgMod(os.path.join(train_image_dir, img1_id))\n",
    "    imgmod2 = ImgMod(os.path.join(train_image_dir, img2_id))\n",
    "    \n",
    "    dtick = 256\n",
    "    n_ticks = imgmod1.shape[1] // dtick + 1\n",
    "    ticks = [i * dtick for i in range(n_ticks)]\n",
    "\n",
    "    slice1 = overlay_tag_slices[img1_overlay_tag]\n",
    "    slice2 = overlay_tag_slices[overlay_tag_pairs[img1_overlay_tag]]\n",
    "\n",
    "    m12 = np.median(np.vstack([imgmod1.parent_rgb[slice1], imgmod2.parent_rgb[slice2]]), axis=(0, 1), keepdims=True).astype(np.uint8)\n",
    "    img1_drop = imgmod1.parent_rgb - m12\n",
    "    img2_drop = imgmod2.parent_rgb - m12\n",
    "        \n",
    "    brightness_level = -100 if np.sum(m12) >= 384 else 100\n",
    "    img1 = imgmod1.channel_shift('L', brightness_level)\n",
    "    img2 = imgmod2.channel_shift('L', brightness_level)\n",
    "    \n",
    "    img1_nine, img2_nine = sdcic.image_image_duplicate_tiles[(img1_id, img2_id)]\n",
    "    img1_mask = img1_nine[overlay_tag_nines_mask[img1_overlay_tag]]\n",
    "    img2_mask = img2_nine[overlay_tag_nines_mask[overlay_tag_pairs[img1_overlay_tag]]]\n",
    "    img1_dups_str = ' '.join(list(map(str, img1_mask)))\n",
    "    img2_dups_str = ' '.join(list(map(str, img2_mask)))\n",
    "    \n",
    "    img1[slice1] = img1_drop[slice1]\n",
    "    img2[slice2] = img2_drop[slice2]\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    ax[0][0].imshow(img1)\n",
    "    ax[0][0].set_title(f'{img1_id} {scores.overlay:7.5f}')\n",
    "    ax[0][0].set_xticks(ticks)\n",
    "    ax[0][0].set_yticks(ticks)\n",
    "\n",
    "    ax[0][1].imshow(img2)\n",
    "    ax[0][1].set_title(f'{img2_id} {min(scores.tile):7.5f}')\n",
    "    ax[0][1].set_xticks(ticks)\n",
    "    ax[0][1].set_yticks(ticks)\n",
    "    \n",
    "    img1[slice1] = imgmod1.parent_rgb[slice1]\n",
    "    img2[slice2] = imgmod2.parent_rgb[slice2]\n",
    "\n",
    "    ax[1][0].imshow(img1)\n",
    "    ax[1][0].set_title(f'({img1_dups_str}) {scores.entropy:7.5f}')\n",
    "    ax[1][0].set_xticks(ticks)\n",
    "    ax[1][0].set_yticks(ticks)\n",
    "\n",
    "    ax[1][1].imshow(img2)\n",
    "    ax[1][1].set_title(f'({img2_dups_str}) {max(scores.pixel)}')\n",
    "    ax[1][1].set_xticks(ticks)\n",
    "    ax[1][1].set_yticks(ticks)\n",
    "    \n",
    "    return ax\n",
    "    \n",
    "def redraw(img1_id, img2_id, img1_overlay_tag, scores):\n",
    "    out.clear_output(True)\n",
    "    with out:\n",
    "        ax = draw_images(img1_id, img2_id, img1_overlay_tag, scores)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_button = Button(\n",
    "    description='Same',\n",
    "    disabled=False,\n",
    "    layout=Layout(flex='1 1 auto', width='auto'), \n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Overlays are the same',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "diff_button = Button(\n",
    "    description='Diff',\n",
    "    disabled=False,\n",
    "    layout=Layout(flex='1 1 auto', width='auto'), \n",
    "    button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Overlays are different',\n",
    "    icon='x'\n",
    ")\n",
    "\n",
    "skip_button = Button(\n",
    "    description='Skip',\n",
    "    disabled=False,\n",
    "    layout=Layout(flex='1 1 auto', width='auto'), \n",
    "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Not sure.  Skip for now.',\n",
    "    icon='?'\n",
    ")\n",
    "\n",
    "def on_same_button_clicked(b):\n",
    "    global img1_id, img2_id, img1_overlay_tag\n",
    "    overlay_labels[(img1_id, img2_id, img1_overlay_tag)] = 1\n",
    "    img1_id, img2_id, img1_overlay_tag, scores = get_next_img_pair()\n",
    "    redraw(img1_id, img2_id, img1_overlay_tag, scores)\n",
    "    \n",
    "def on_diff_button_clicked(b):\n",
    "    global img1_id, img2_id, img1_overlay_tag\n",
    "    overlay_labels[(img1_id, img2_id, img1_overlay_tag)] = 0\n",
    "    img1_id, img2_id, img1_overlay_tag, scores = get_next_img_pair()\n",
    "    redraw(img1_id, img2_id, img1_overlay_tag, scores)\n",
    "    \n",
    "def on_skip_button_clicked(b):\n",
    "    global img1_id, img2_id, img1_overlay_tag\n",
    "    img1_id, img2_id, img1_overlay_tag, scores = get_next_img_pair()\n",
    "    redraw(img1_id, img2_id, img1_overlay_tag, scores)\n",
    "\n",
    "same_button.on_click(on_same_button_clicked)\n",
    "diff_button.on_click(on_diff_button_clicked)\n",
    "skip_button.on_click(on_skip_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out = Output()\n",
    "buttons_3 = Box(children=[same_button, diff_button, skip_button], layout=box_layout)\n",
    "display(VBox([out, buttons_3]))\n",
    "\n",
    "img1_id, img2_id, img1_overlay_tag, overlay_scores = get_next_img_pair()\n",
    "with out:\n",
    "    ax = draw_images(img1_id, img2_id, img1_overlay_tag, overlay_scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdcic.tile_entropy_grids['00374ccfa.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdcic.tile_entropy_grids['218bb7055.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(overlay_labels), 100*len(overlay_labels)/len(duplicate_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in overlay_labels.items():\n",
    "    continue\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del overlay_labels[('b887a184a.jpg', 'ca57c33ce.jpg', '0010')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_labels[('0ee790381.jpg', '2f6c0deaa.jpg', '1020')] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_duplicate_truth(duplicate_truth_file, overlay_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we create image_duplicate_tiles.txt, lets check to see how many duplicate tiles we actually have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_duplicate_tiles = read_image_duplicate_tiles(image_duplicate_tiles_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_tiles = []\n",
    "dup_hashes = {}\n",
    "dup_files = []\n",
    "dup_counts = {}\n",
    "for img_id, img_dup9 in sdcic.image_duplicate_tiles.items():\n",
    "    img = None\n",
    "    c0 = Counter(img_dup9)\n",
    "    for i, c in c0.items():\n",
    "        if c == 1:\n",
    "            continue\n",
    "        for ii in np.where(img_dup9 == i)[0]:\n",
    "            new_hash = sdcic.tile_md5hash_grids[img_id][ii]\n",
    "            if new_hash in dup_hashes:\n",
    "                dup_counts[new_hash] += 1\n",
    "                if img_id not in dup_hashes[new_hash]:\n",
    "                    dup_hashes[new_hash][img_id] = []\n",
    "                dup_hashes[new_hash][img_id].append(ii)\n",
    "            else:\n",
    "                dup_counts[new_hash] = 1\n",
    "                dup_hashes[new_hash] = {}\n",
    "                dup_hashes[new_hash][img_id] = []\n",
    "                dup_hashes[new_hash][img_id].append(ii)\n",
    "                dup_files.append(img_id)\n",
    "                img = sdcic.get_img(img_id)\n",
    "                new_tile = sdcic.get_tile(img, ii)\n",
    "                dup_tiles.append(new_tile)\n",
    "                print(len(dup_files)-1, new_hash, img_id, ii)\n",
    "dup_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ii, dup_tile in enumerate(dup_tiles):\n",
    "    print(ii)\n",
    "    print(dup_tile[2, 2], dup_tile[2, -2])\n",
    "    print(dup_tile[-2, 2], dup_tile[-2, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "img1 = cv2.cvtColor(cv2.imread(os.path.join(train_image_dir, dup_files[0])), cv2.COLOR_BGR2RGB)\n",
    "ax1.imshow(img1)\n",
    "img2 = cv2.cvtColor(cv2.imread(os.path.join(train_image_dir, dup_files[5])), cv2.COLOR_BGR2RGB)\n",
    "ax2.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "img1 = cv2.cvtColor(cv2.imread(os.path.join(train_image_dir, dup_files[1])), cv2.COLOR_BGR2RGB)\n",
    "ax1.imshow(img1)\n",
    "img2 = cv2.cvtColor(cv2.imread(os.path.join(train_image_dir, dup_files[2])), cv2.COLOR_BGR2RGB)\n",
    "ax2.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "img1 = cv2.cvtColor(cv2.imread(os.path.join(train_image_dir, dup_files[3])), cv2.COLOR_BGR2RGB)\n",
    "ax1.imshow(img1)\n",
    "img2 = cv2.cvtColor(cv2.imread(os.path.join(train_image_dir, dup_files[4])), cv2.COLOR_BGR2RGB)\n",
    "ax2.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_tile = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "white_tile = black_tile + 255\n",
    "blue_tile = np.copy(black_tile) \n",
    "blue_tile[:, :, 0] = 255\n",
    "red_tile = np.copy(black_tile) \n",
    "red_tile[:, :, 2] = 255\n",
    "color_tiles = [black_tile, white_tile, blue_tile, red_tile]\n",
    "for color_tile in color_tiles:\n",
    "    print(img_hash.blockMeanHash(color_tile, mode=0)[0])\n",
    "    print(hashlib.md5(color_tile.tobytes()).hexdigest())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_images = ['03ffa7680.jpg', '8d5521663.jpg', '5a70ef013.jpg', '9a2f9d347.jpg', '37a912dca.jpg', '4add7aa1d.jpg', '3db3ef7cc.jpg', '73fec0637.jpg', '7df214d98.jpg', 'c2955cd21.jpg', 'de018b2a8.jpg', '8ce769141.jpg', 'fc0e22a0a.jpg', '770c46cd4.jpg', 'd6e432b79.jpg', 'd5d1b6fb8.jpg', '0e4d7dd93.jpg', '9ddeed533.jpg', 'addc11de0.jpg', '65418dfe4.jpg', '119d6a3d6.jpg', '1b287c905.jpg', 'b264b0f96.jpg', '996f92939.jpg', 'e5c3b1f59.jpg']\n",
    "fig, ax = plt.subplots(5, 5, figsize=(15, 15))\n",
    "for i, img_id in enumerate(black_images):\n",
    "    img = cv2.cvtColor(cv2.imread(os.path.join(train_image_dir, img_id)), cv2.COLOR_BGR2RGB)\n",
    "    ax[i // 5, i % 5].imshow(img)\n",
    "    ax[i // 5, i % 5].set_title(img_id)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out a decision tree classifier for dup_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "X = []\n",
    "Y = []\n",
    "for (img1_id, img2_id, img1_overlay_tag), is_dup in dup_truth.items():\n",
    "    if (img1_id, img2_id) not in sdcic.overlay_image_maps:\n",
    "#         print((img1_id, img2_id, img1_overlay_tag))\n",
    "        continue\n",
    "    overlay_maps = sdcic.overlay_image_maps[(img1_id, img2_id)]\n",
    "    if img1_overlay_tag not in overlay_maps:\n",
    "#         print(img1_overlay_tag)\n",
    "        continue\n",
    "    (overlay_score, tile_scores, pixel_scores, entropy_score) = overlay_maps[img1_overlay_tag]\n",
    "    L.append((img1_id, img2_id, img1_overlay_tag))\n",
    "    X.append([\n",
    "#         min(tile_scores), \n",
    "        max(pixel_scores), \n",
    "        entropy_score])\n",
    "    Y.append([is_dup])\n",
    "\n",
    "L = np.array(L)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "# X = [[0, 0], [1, 1]]\n",
    "# Y = [0, 1]\n",
    "print(len(X))\n",
    "print(len(Y), sum(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=['max(pixel)', 'entropy'], \n",
    "                                filled=True, rounded=True, special_characters=True, leaves_parallel=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = clf.apply(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = np.where(all_nodes == 4)\n",
    "idx = 0\n",
    "print(L[nodes][idx], Y[nodes][idx], X[nodes][idx])\n",
    "print(sdcic.overlay_image_maps[(L[nodes][idx][0], L[nodes][idx][1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "['1ebdf2f08.jpg', 'b1bfb768c.jpg', '0012'] [1] [91.          0.99781223]\n",
    "{'0012': (\n",
    "    1.0, \n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "    [0, 0, 0, 91, 0, 0], \n",
    "    0.9978122327282525)}\n",
    "\n",
    "['2323bf875.jpg', 'b5da61fce.jpg', '0021'] [1] [2.05663500e+06 9.98277186e-01]\n",
    "{'0021': (\n",
    "    0.9913398692810458, \n",
    "    [0.984313725490196, 1.0, 1.0, 0.996078431372549, 0.9686274509803922, 0.9990196078431373], \n",
    "    [2043928, 1911748, 2056635, 2046531, 2053888, 2019553], \n",
    "    0.9982771859297246)}\n",
    "\n",
    "['468bf9178.jpg', '6090b3a8b.jpg', '1022'] [1] [1.30900000e+03 9.97640283e-01]\n",
    "{'1022': (\n",
    "    1.0, \n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "    [721, 1259, 1278, 1309, 753, 358], \n",
    "    0.9976402831644123)}\n",
    "\n",
    "['0ef6cd331.jpg', 'e6a6f80cd.jpg', '1022'] [0] [1.72270000e+04 9.98394555e-01]\n",
    "{'1022': (\n",
    "    0.9866830065359478, \n",
    "    [0.9965686274509804, 1.0, 0.9529411764705882, 1.0, 1.0, 0.9705882352941176], \n",
    "    [17227, 0, 1561, 0, 0, 16281], \n",
    "    0.9983945550575503)}\n",
    "\n",
    "['d4f0aaa70.jpg', 'd84d4a78a.jpg', '0012'] [0] [5.95230000e+04 9.98578088e-01]\n",
    "{'0012': (\n",
    "    0.9937704248366013, \n",
    "    [0.96875, 1.0, 1.0, 0.9938725490196079, 1.0, 1.0], \n",
    "    [59523, 0, 0, 43375, 0, 0], \n",
    "    0.9985780878066167)}\n",
    "\n",
    "# 4\n",
    "['de6fb187d.jpg', 'ea6dc23b7.jpg', '1021'] [1] [223.           0.99544613]\n",
    "{'1021': (\n",
    "    1.0, \n",
    "    [1.0, 1.0, 1.0, 1.0], \n",
    "    [0, 223, 0, 2], \n",
    "    0.9954461317609192)}\n",
    "\n",
    "['c3193fb05.jpg', 'cc68e7818.jpg', '0112'] [0] [2.16300000e+04 9.98311792e-01]\n",
    "{'0112': (\n",
    "    0.9834865196078432, \n",
    "    [1.0, 0.9743872549019608, 1.0, 0.9595588235294118], \n",
    "    [0, 21630, 0, 5512], \n",
    "    0.9983117921966159)}\n",
    "\n",
    "['cd3c59923.jpg', 'efdd03319.jpg', '1021'] [0] [6.70246000e+05 9.99894307e-01]\n",
    "{'1021': (\n",
    "    0.9917279411764706, \n",
    "    [0.9669117647058824, 1.0, 1.0, 1.0], \n",
    "    [670246, 0, 0, 0], \n",
    "    0.9998943068660762)}\n",
    "\n",
    "# 3\n",
    "['2f6c0deaa.jpg' 'e44a4f5b0.jpg' '0222'] [1] [24.          0.99509307]\n",
    "{'0222': (1.0, [1.0, 1.0, 1.0], [24, 0, 0], 0.9950930748673227)}\n",
    "\n",
    "['204906e27.jpg' '892a69b4b.jpg' '0002'] [1] [6.31644000e+05 9.97614902e-01]\n",
    "{'0002': (0.9787173202614379, [0.9685049019607843, 1.0, 0.9676470588235294], [588063, 631644, 588677], 0.9976149023387387)}\n",
    "\n",
    "['4c56d2f00.jpg' 'dcd94e973.jpg' '2022'] [1] [6.31635000e+05 9.97534103e-01]\n",
    "{'2022': (0.9787173202614379, [0.9685049019607843, 1.0, 0.9676470588235294], [588032, 631635, 588642], 0.9975341029628989)}\n",
    "\n",
    "['b645cd49b.jpg' 'f2e554691.jpg' '2022'] [1] [3.76847000e+05 9.96659721e-01]\n",
    "{'2022': (0.986233660130719, [1.0, 0.9872549019607844, 0.9714460784313725], [376847, 328985, 314896], 0.996659721397496)}\n",
    "\n",
    "['b998c7415.jpg' 'd4d26f700.jpg' '2022'] [1] [3.76847000e+05 9.96680501e-01]\n",
    "{'2022': (0.986233660130719, [1.0, 0.9872549019607844, 0.9714460784313725], [376847, 328986, 314908], 0.9966805008347782)}\n",
    "\n",
    "['0ef6cd331.jpg' '3a9e579aa.jpg' '2022'] [0] [1.62810000e+04 9.98394555e-01]\n",
    "{'2022': (0.9901960784313726, [1.0, 1.0, 0.9705882352941176], [0, 0, 16281], 0.9983945550575503)}\n",
    "\n",
    "['0ef6cd331.jpg' '813c8ec35.jpg' '0222'] [0] [1.79442000e+05 9.98195859e-01]\n",
    "{'0222': (0.9838643790849673, [1.0, 1.0, 0.9515931372549019], [0, 0, 179442], 0.9981958590284148)}\n",
    "\n",
    "['813c8ec35.jpg' 'caa94ffc3.jpg' '0020'] [0] [1.76759000e+05 9.99834742e-01]\n",
    "{'0020': (0.9944035947712418, [1.0, 1.0, 0.9832107843137254], [0, 0, 176759], 0.9998347424252452)}\n",
    "\n",
    "['0256ef90d.jpg' '46da51931.jpg' '0020'] [0] [3.70260000e+05 9.99319673e-01]\n",
    "{'0020': (0.9838643790849673, [1.0, 0.986764705882353, 0.964828431372549], [0, 135489, 370260], 0.9993196729978772)}\n",
    "\n",
    "['a61b3e245.jpg' 'd84d4a78a.jpg' '2022'] [0] [2.59134100e+06 9.99175738e-01]\n",
    "{'2022': (0.985702614379085, [0.9584558823529412, 0.9986519607843137, 1.0], [2591341, 38980, 0], 0.9991757382992734)}\n",
    "\n",
    "# 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the performance of DupNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = load_checkpoint('out/dup_model.last.pth')\n",
    "model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_pair(img1_id, img2_id, img1_overlay_tag):\n",
    "    \n",
    "    imgmod1 = ImgMod(os.path.join(train_image_dir, img1_id))\n",
    "    imgmod2 = ImgMod(os.path.join(train_image_dir, img2_id))\n",
    "    dcnn_scores_raw = gen_dcnn_scores(imgmod1.parent_rgb, imgmod2.parent_rgb, img1_overlay_tag, model)\n",
    "    print(dcnn_scores_raw)\n",
    "    \n",
    "    dtick = 256\n",
    "    n_ticks = imgmod1.shape[1] // dtick + 1\n",
    "    ticks = [i * dtick for i in range(n_ticks)]\n",
    "\n",
    "    slice1 = overlay_tag_slices[img1_overlay_tag]\n",
    "    slice2 = overlay_tag_slices[overlay_tag_pairs[img1_overlay_tag]]\n",
    "\n",
    "    m12 = np.median(np.vstack([imgmod1.parent_rgb[slice1], imgmod2.parent_rgb[slice2]]), axis=(0, 1), keepdims=True).astype(np.uint8)\n",
    "    \n",
    "    brightness_level = -100 if np.sum(m12) >= 384 else 100\n",
    "    img1 = imgmod1.channel_shift('L', brightness_level)\n",
    "    img2 = imgmod2.channel_shift('L', brightness_level)\n",
    "    \n",
    "    img1_drop = imgmod1.parent_rgb - m12\n",
    "    img2_drop = imgmod2.parent_rgb - m12\n",
    "    \n",
    "    img1[slice1] = img1_drop[slice1]\n",
    "    img2[slice2] = img2_drop[slice2]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    \n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title(f'{img1_id}')\n",
    "    ax1.set_xticks(ticks)\n",
    "    ax1.set_yticks(ticks)\n",
    "    \n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title(f'{img2_id}')\n",
    "    ax2.set_xticks(ticks)\n",
    "    ax2.set_yticks(ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_pair('cd3c59923.jpg', 'efdd03319.jpg', '1021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_truth1 = {}\n",
    "for key, is_dup in dup_truth.items():\n",
    "    if is_dup:\n",
    "        continue\n",
    "    dup_truth1[key] = is_dup\n",
    "len(dup_truth1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ii = 1\n",
    "dup_dict = {}\n",
    "for (img1_id, img2_id, img1_overlay_tag), is_dup in dup_truth1.items():\n",
    "    ii += 1\n",
    "    if img1_id > img2_id:\n",
    "        img1_id, img2_id = img2_id, img1_id\n",
    "        img1_overlay_tag = overlay_tag_pairs[img1_overlay_tag]\n",
    "\n",
    "    if (img1_id, img2_id, img1_overlay_tag) in dup_dict:\n",
    "        continue\n",
    "\n",
    "    img1 = cv2.imread(os.path.join(train_image_dir, img1_id))\n",
    "    img2 = cv2.imread(os.path.join(train_image_dir, img2_id))\n",
    "\n",
    "    entropy_score = get_entropy_score(img1_id, img2_id, img1_overlay_tag, sdcic.tile_entropy_grids)\n",
    "    overlay_score = get_overlay_score(img1_id, img2_id, img1_overlay_tag, sdcic.tile_bm0hash_grids)\n",
    "    tile_scores = get_tile_scores(img1_id, img2_id, img1_overlay_tag, sdcic.tile_bm0hash_grids)\n",
    "    pixel_scores = gen_pixel_scores(img1, img2, img1_overlay_tag)\n",
    "    dcnn_scores_raw = gen_dcnn_scores(img1, img2, img1_overlay_tag, model)\n",
    "    dcnn_scores = (dcnn_scores_raw > 0.5) * 1\n",
    "\n",
    "    dup_dict[(img1_id, img2_id, img1_overlay_tag)] = (is_dup, overlay_score, min(tile_scores))\n",
    "\n",
    "    if is_dup == np.max(dcnn_scores) == 0:\n",
    "        continue\n",
    "\n",
    "    if is_dup == np.min(dcnn_scores) == 1:\n",
    "        continue\n",
    "\n",
    "#     print(f\"{ii:>3} {img1_id} {img2_id}               {is_dup} {np.min(dcnn_scores)} {image_score:6.4f}, {min(tile_scores):6.4f}]\", pixel_scores, dcnn_scores_raw)\n",
    "    print(f\"{ii:>3} {img1_id} {img2_id} {img1_overlay_tag} {is_dup} {entropy_score} {image_score}\")\n",
    "    print(pixel_scores)\n",
    "#     print(dcnn_scores)\n",
    "    print(dcnn_scores_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img1_id, img2_id, img1_overlay_tag), (is_dup, image_score, tiles_score) in dup_dict.items():\n",
    "#     if is_dup < 0:\n",
    "#         continue\n",
    "    \n",
    "    filename = os.path.join('temp', f\"{img1_id}_{img2_id}\")\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "\n",
    "    img1 = cv2.cvtColor(cv2.imread(os.path.join(train_image_dir, img1_id)), cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.cvtColor(cv2.imread(os.path.join(train_image_dir, img2_id)), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    overlay_score = get_overlay_score(img1_id, img2_id, img1_overlay_tag, sdcic.tile_bm0hash_grids)\n",
    "    tile_scores = get_tile_scores(img1_id, img2_id, img1_overlay_tag, sdcic.tile_bm0hash_grids)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    \n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title(f'{img1_id} ({overlay_score:7.5f})')\n",
    "    \n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title(f'{img2_id} ({min(tile_scores):7.5f})')\n",
    "    \n",
    "    fig.savefig(filename)\n",
    "    fig.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_pair_with_mask(img1_id, img2_id, img1_overlay_tag):\n",
    "#     if is_dup == 0:\n",
    "#         continue\n",
    "#     has_mask = 0\n",
    "#     if os.path.exists(os.path.join(train_mask_dir, img1_id)):\n",
    "#         has_mask += 1\n",
    "#     if os.path.exists(os.path.join(train_mask_dir, img2_id)):\n",
    "#         has_mask += 1\n",
    "#     if has_mask == 0:\n",
    "#         continue\n",
    "    imgmod1 = ImgMod(os.path.join(train_image_dir, img1_id))\n",
    "    imgmod2 = ImgMod(os.path.join(train_image_dir, img2_id))\n",
    "    \n",
    "    dtick = 256\n",
    "    n_ticks = imgmod1.shape[1] // dtick + 1\n",
    "    ticks = [i * dtick for i in range(n_ticks)]\n",
    "\n",
    "    img1 = imgmod1.parent_rgb\n",
    "    img2 = imgmod2.parent_rgb\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    \n",
    "    ax1.imshow(img1)\n",
    "#     if os.path.exists(os.path.join(train_mask_dir, img1_id)):\n",
    "#         mask1 = cv2.imread(os.path.join(train_mask_dir, img1_id))\n",
    "#         mask1 = cv2.cvtColor(mask1, cv2.COLOR_BGR2RGB)\n",
    "#         ax1.imshow(mask1[..., 0], alpha=0.5)\n",
    "    ax1.set_title(f'{img1_id}')\n",
    "    ax1.set_xticks(ticks)\n",
    "    ax1.set_yticks(ticks)\n",
    "    \n",
    "    ax2.imshow(img2)\n",
    "#     if os.path.exists(os.path.join(train_mask_dir, img2_id)):\n",
    "#         mask2 = cv2.imread(os.path.join(train_mask_dir, img2_id))\n",
    "#         mask2 = cv2.cvtColor(mask2, cv2.COLOR_BGR2RGB)\n",
    "#         ax2.imshow(mask2[..., 0], alpha=0.5)\n",
    "    ax2.set_title(f'{img2_id}')\n",
    "    ax2.set_xticks(ticks)\n",
    "    ax2.set_yticks(ticks)\n",
    "    \n",
    "#     fig.savefig(os.path.join('temp', f\"{img1_id}_{img2_id}_mask.jpg\"));\n",
    "#     fig.clear();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img1_id, img2_id, img1_overlay_tag), (is_dup, tiles_score, tiles_score1) in dup_dict.items():\n",
    "    plot_image_pair_with_mask(img1_id, img2_id, img1_overlay_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key, value in sorted(dup_dict.items(), key=lambda x: x[1][2]):\n",
    "    has_mask = []\n",
    "    if value[0]:\n",
    "        if os.path.exists(os.path.join(train_mask_dir, key[0])):\n",
    "            has_mask.append(1)\n",
    "        else:\n",
    "            has_mask.append(0)\n",
    "        if os.path.exists(os.path.join(train_mask_dir, key[1])):\n",
    "            has_mask.append(1)\n",
    "        else:\n",
    "            has_mask.append(0)\n",
    "        print(f'{key[0]}, {has_mask[0]}, {key[1]}, {has_mask[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
