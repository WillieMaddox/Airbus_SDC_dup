{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is helpful for tracking the training process when train_dupnet.ipynb is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sdcdup.features.image_features import SDCImageContainer\n",
    "from sdcdup.utils import create_dataset_from_tiles\n",
    "from sdcdup.utils import reversed_recombined_holt_winters\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 16\n",
    "BIGGEST_SIZE = 20\n",
    "plt.rc('font', size=BIGGEST_SIZE)         # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGEST_SIZE)    # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGEST_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=BIGGER_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=BIGGER_SIZE)   # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)   # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGEST_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# SENDTOENV\n",
    "train_image_dir = 'data/raw/train_768/'\n",
    "image_md5hash_grids_file = 'data/interim/image_md5hash_grids.pkl'\n",
    "image_bm0hash_grids_file = 'data/interim/image_bm0hash_grids.pkl'\n",
    "image_cm0hash_grids_file = 'data/interim/image_cm0hash_grids.pkl'\n",
    "image_greycop_grids_file = 'data/interim/image_greycop_grids.pkl'\n",
    "image_entropy_grids_file = 'data/interim/image_entropy_grids.pkl'\n",
    "image_issolid_grids_file = 'data/interim/image_issolid_grids.pkl'\n",
    "image_shipcnt_grids_file = 'data/interim/image_shipcnt_grids.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_filename = 'data/processed/full_SDC_dataset_from_tiles.csv'\n",
    "if os.path.exists(full_dataset_filename):\n",
    "    df = pd.read_csv(full_dataset_filename)\n",
    "    full_dataset = list(zip(*[df[c].values.tolist() for c in df]))\n",
    "else:\n",
    "    sdcic = SDCImageContainer()\n",
    "    sdcic.preprocess_image_properties(\n",
    "        image_md5hash_grids_file,\n",
    "        image_bm0hash_grids_file,\n",
    "        image_cm0hash_grids_file,\n",
    "        image_greycop_grids_file,\n",
    "        image_entropy_grids_file,\n",
    "        image_issolid_grids_file)\n",
    "    full_dataset = create_dataset_from_tiles(sdcic)\n",
    "\n",
    "print(len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "fname = 'dup_model.2019_0727_2246.metrics.csv'\n",
    "fname = 'dup_model.2019_0730_1923.metrics.csv'\n",
    "fname = 'dup_model.2019_0802_2209.metrics.csv'\n",
    "df_stats = pd.read_csv('models/' + fname)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6))\n",
    "# span = 111//30 = 3\n",
    "# span = epochs // 100\n",
    "span = 2\n",
    "field_ylim = {\n",
    "    'loss': (0, 0.005), \n",
    "    'brier_loss': (0, None),\n",
    "    'focal_loss': (0, None), \n",
    "    'focal_loss_1': (0, 5), \n",
    "    'focal_loss_2': (0, 5), \n",
    "    'dice_coef_loss': (0, None), \n",
    "    'soft_dice_loss': (0, None),\n",
    "    'binary_crossentropy': (0, None),\n",
    "    'acc': (None, 1), \n",
    "    'iou': (None, 1), \n",
    "    'mean_iou': (None, 1), \n",
    "    'f2': (None, 1), \n",
    "    'fbeta': (None, 1), \n",
    "    'soft_dice_coef': (None, 1),\n",
    "    'hard_dice_coef': (None, 1), \n",
    "    'hard_dice_coef2': (None, 1), \n",
    "    'hard_dice_coef_ch1': (None, 1),\n",
    "} \n",
    "stats_fields = ('loss', 'acc')\n",
    "filename = []\n",
    "for j in range(ncols):\n",
    "    field = stats_fields[j]\n",
    "#     filename.append(field)\n",
    "\n",
    "    stats_dict = {\n",
    "        'train_'+field: df_stats['train_'+field],\n",
    "        'val_'+field: df_stats['val_'+field],\n",
    "        'train_'+field+' (hw)': reversed_recombined_holt_winters(np.array(df_stats['train_'+field]), span=span),\n",
    "        'val_'+field+' (hw)': reversed_recombined_holt_winters(np.array(df_stats['val_'+field]), span=span)\n",
    "    }\n",
    "    alphas = {\n",
    "        'train_'+field: 0.3,\n",
    "        'val_'+field: 0.3,\n",
    "        'train_'+field+' (hw)': 1,\n",
    "        'val_'+field+' (hw)': 1\n",
    "    }\n",
    "    ax = axes[j]\n",
    "    legend_labels = []\n",
    "    for key, value in stats_dict.items():\n",
    "        ax.plot(value, alpha=alphas[key])\n",
    "        legend_labels.append(key)\n",
    "    # Special case for f2 callback.\n",
    "    if field == 'fbeta' and 'val_f2' in df_stats:\n",
    "        ax.plot(df_stats['val_f2'])\n",
    "        legend_labels.append('val_f2')\n",
    "    ax.set_title(field)\n",
    "    ax.set_xlabel(r'epoch')\n",
    "    ax.set_xlim((0, len(df_stats['epoch'])))\n",
    "    ax.set_ylim(field_ylim[field])\n",
    "    ax.grid(True)\n",
    "    ax.legend(legend_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "# filename = '-'.join(filename)\n",
    "# plt.savefig(out_dir + filename + '.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6))\n",
    "\n",
    "ignored_runs = [\n",
    "    \"dup_model.2019_0709_0149.metrics.csv\",\n",
    "    \"dup_model.2019_0714_0200.metrics.csv\",\n",
    "    \"dup_model.2019_0720_0155.metrics.csv\",\n",
    "    \"dup_model.2019_0724_0938.metrics.csv\",\n",
    "    \"dup_model.2019_0727_2246.metrics.csv\",\n",
    "#     \"dup_model.2019_0730_1923.metrics.csv\",\n",
    "    \"dup_model.2019_0731_2310.metrics.csv\",\n",
    "    \"dup_model.2019_0801_0954.metrics.csv\",\n",
    "    \"dup_model.2019_0801_2010.metrics.csv\",\n",
    "#     \"dup_model.2019_0802_0708.metrics.csv\",\n",
    "    \"dup_model.2019_0805_0015.metrics.csv\",\n",
    "]\n",
    "csv_files = []\n",
    "for fname in os.listdir('models'):\n",
    "    if fname.endswith('metrics.csv') and fname not in ignored_runs:\n",
    "        csv_files.append(fname)\n",
    "\n",
    "legend_labels = []\n",
    "dataframes = []\n",
    "for fname in sorted(csv_files):\n",
    "    dataframes.append(pd.read_csv('models/' + fname))\n",
    "    legend_labels.append(fname.split('.')[-3])\n",
    "\n",
    "xlabels = ('epoch', 'time')\n",
    "xmax = (100, 100000)\n",
    "for j in range(ncols):\n",
    "#     xmax = 0\n",
    "    xlabel = xlabels[j]\n",
    "    ax = axes[j]\n",
    "    for df in dataframes:\n",
    "#         value = df.train_loss\n",
    "#         ax.plot(df[xlabel], value)\n",
    "#         value = reversed_recombined_holt_winters(np.array(df.val_loss), span=2)\n",
    "        value = df.val_loss\n",
    "        ax.plot(df[xlabel], value)\n",
    "#         xmax = max(xmax, max(df[xlabel]))\n",
    "\n",
    "    ax.set_title('val loss')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_xlim((0, xmax[j]))\n",
    "    ax.set_ylim((0, 0.005))\n",
    "#     ax.grid(True)\n",
    "    ax.legend(legend_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"dup_model.2019_0802_0708.56.avl.csv\"\n",
    "df_avl = pd.read_csv('models/' + fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avl.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avl['ages'].hist(bins=df_avl['ages'].max()//100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avl['visits'].hist(bins=df_avl['visits'].max());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_name = \"2019_0802_2209\"\n",
    "csv_files = []\n",
    "for fname in os.listdir('models'):\n",
    "    if fname.endswith('avl.csv') and run_name in fname:\n",
    "        csv_files.append(fname)\n",
    "\n",
    "# dataframes = []\n",
    "for fname in sorted(csv_files):\n",
    "    model_name, date_time, epoch, val_loss0, val_loss1, ds_type, file_type = fname.split('.')\n",
    "    val_loss = '.'.join([val_loss0, val_loss1])\n",
    "    df = pd.read_csv('models/' + fname)\n",
    "    print(f\"{int(epoch):>2} | {df.ages.max():>5}, ({df.visits.min()}, {df.visits.max():>2}), {df.losses.max():>7.4f}, {df.losses.sum():>12.2f}, {float(val_loss):<8.6f}\")\n",
    "#     dataframes.append((int(epoch), df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}